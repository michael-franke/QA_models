//  -------------------
// | utility functions |
//  -------------------

// poor person's barplot emulation for terminal
var terminalViz = function(dist, precisionLevel) {

  var unfiltered_support = dist.support();
  var unfiltered_probs   = map(function(x) {
    return 1*dist.score(x).toPrecision(precisionLevel)}, dist.support())

  var unsorted_probs   = filter(function(x) {return Math.exp(x) > 0},
                                unfiltered_probs)
  var unsorted_support = filter(function(x) {return Math.exp(dist.score(x)) > 0},
                                unfiltered_support )

  var sorted_probs     = sort(unsorted_probs);
  var sortFunction = function(x) {
    return -1*dist.score(x).toPrecision(precisionLevel)
  }
  var sorted_support   = sortOn(unsorted_support, sortFunction)
  var max_length_element = _.max(map(function(e) {e.length}, sorted_support));
  var scores = map(function(x) {
    return 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
  }, sorted_support)
  var maxScore =  _.max(map(function(e) {e}, scores));
  map(
    function(x) {
      var score = 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
      console.log(" ",
                  _.padEnd(x, max_length_element, " "),
                  ": ",
                  _.padEnd(_.repeat('*', score*20), 21),
                  score
                 )}
    , sorted_support)
  return "  ===viz==="
}

var butLast = function(xs){
  return xs.slice(0, xs.length-1);
};

var KL = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return (probP == 0.0 ? 0.0 :
            probQ == 0.0 ? 1000000:
            probP * (scoreP - scoreQ));
  }, values));
};

var SumSquares = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return ((scoreP - scoreQ)^2);
  }, values));
};

var powerset = function(set) {
  if (set.length == 0) {
    return [[]];
  } else {
    var rest = powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

// replace empty string with word 'nothing'
var replaceEmptyListWithStringNothing = function(set) {
  _.concat(filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    set
  ), "nothing");
}


//  -----------------------------------------------
// | helper functions for preparing context models |
//  -----------------------------------------------
//
// Context models are built from a set of 'atoms', e.g.,
// the whole set of single entities in the domain.
//
// The function 'prepareContextSets' returns a dictionary
// with:
// * the set of all world states (power set of atoms),
// * the set of all licensed responses for R1 to a WH question
// * the set of all licensed responses for R1 to a polar question
//
// The function 'meaningFunction' is the generic semantic function
// to be used in (non-minimal) cases where more than one element/item
// can be true/present.
// TODO: check if the meaning function for the minimal context models
// can be subsumed under this one.

var prepareContextSets = function(atoms) {

  // The function 'makePowerSet' takes a list of atoms as input and
  // returns an array of strings describing each world state
  // (e.g., which items are present).
  var makePowerSet = function (atoms) {
    // create string representation of all subsets of atoms
    var setWithEmptyListElement = map(
      function(v){return v.join('+');},
      // add possibility to encode background knowledge of number k of available goods
      filter(function(x) {return x.length <= 8}, powerset(atoms))
    );
    return(replaceEmptyListWithStringNothing(setWithEmptyListElement));
  };

  // The function 'makeR1polarResponses' takes the atoms and the power set and
  // creates all licensed R1 responses to a polar question.
  var makeR1polarResponses = function(atoms, powerSet) {

    var sampleR1PolarResponses = Infer(
      {method: 'enumerate'},
      function() {
        var yesNoPart = uniformDraw(["yes", "no"]);
        var itemPart = uniformDraw(_.concat(filter(
          function(x) {
            if (x != "") {
              return x
            }
          },
          powerSet
        ), "---", "nothing"));
        return [yesNoPart, itemPart].join(".")
      }
    )

    // exclude utterance 'no, we have ... exhaustive list of everything'
    // and 'yes, we have nothing'
    var x = map(function(a) {a == atoms[0] ? a : a + "+" }, atoms.reverse())
    var contradiction = reduce(function(a, acc) { acc + a }, "no.", x)

    var R1PolarResponses = filter(
      function(r) {
        // some responses can never be true
        r != "yes.nothing" && r != contradiction
      },
      sampleR1PolarResponses.support());
    return(R1PolarResponses)
  }

  var powerSet = makePowerSet(atoms);

  // return powerset, R1 licencesed responses
  var out = {
    'atoms'            : atoms,
    'powerSet'         : powerSet,
    'R1WHResponses'    : powerSet,
    'R1PolarResponses' : makeR1polarResponses(atoms, powerSet)
  }
  return(out)
}

////////////////// extension for handling basic level questions for secondary goal extension /////////////////////////////
var meaningFunctionBasicQ = function(world, question, response) {

  // meaning of literals / atoms
  var meaning_atomic = function(world, question, response) {
    // console.log(" *** now evaluating response: part ", response)
    if(response == '' || response == "---") {
      // assume silence has null meaning
      return true;
    }
    if(response == "nothing") {
      if(world == "nothing") {
        return true;
      }
    }
    if(world == "nothing" && question.type == 'wh') {
      return response == "nothing";
    }
    if(question.type == 'single-item') {
      return (
        response == 'yes' ? _.intersection(_.flatten(map(function(x){return x.split("_")}, world.split('+'))), question.queried).length > 0 :
          response == 'no' ?  _.intersection(_.flatten(map(function(x){return x.split("_")}, world.split('+'))), question.queried).length == 0 :
          all(function(item) {
            return _.includes(world.split('+'), item);
          }, response.split('+')));
      } else if(question.type == 'wh') {
        // assume response is true when the shop contains every mentioned item
        return all(function(item) {
          return _.includes(world.split('+'), item);
        }, response.split('+'));
      } else {
        return console.error('question type not yet supported: ' + question.type);
      }
  }
  return all(
    function (r) {
      meaning_atomic(world, question, r)
    },
    _.split(response, '.')
  )
}

// response cost is proportional to length in words
var cost = function(response,params) {
  return _.includes(response, "---") ? 0 : params.costWeight * (response.split('+').length);
};
//////////////////////////////////////////////////////////////////////

//  --------------------------
// | preparing context models |
//  --------------------------

///////////////////////// PTs context extensions for cost-sensitive QA reasoning /////////////////
// the alternatives are constructed by <property>_<item>
// depending on context, the speaker may or may not reason about the property and respective cost
var costAtoms = ['noParking_coffee', 'parking_coffee'] //
var costR1WHResponses = ['noParking_coffee+parking_coffee'] // 
// construct the set of possible worlds
var costPowerSet = filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    map(
      function(v){return v.join('+');}, powerset(costAtoms)
    )
)
// construct possible responses
var costR1PolarResponses = prepareContextSets(costAtoms).R1PolarResponses;

var costContext = {
  name : "costContext",
  worlds : costPowerSet,
  actions: _.concat(costAtoms, "nothing"), // assume 'nothing' is just doing nothing
  questions: [
    {type: 'single-item', queried: ['coffee'], text: 'Coffee?'}, // assume this is the only possible question for coffee
    {type: 'wh', queried: costAtoms, text: 'Which elements?'},
  ],
  questionerBeliefs: Categorical({vs: costPowerSet}),
  R0PriorOverWorlds: Delta({v: 'parking_coffee+noParking_coffee'}),
  R1PriorOverWorlds: Delta({v: 'parking_coffee+noParking_coffee'}), 
  decisionProblem: function(w, a) { // neutral preference for any coffee shop
      return _.includes(w, a) ?
                (a == 'parking_coffee'    ? 7   : 7 ): 
                 a == 'nothing' ? 0 : -Infinity;
  },

  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            costR1WHResponses : costR1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunctionBasicQ // extends the original tso one to incorporate the basic level question
};

// extended context with parking preferences
// precise util values are actually read form user's intput and override these
var costContextParking = extend(
  costContext,
  {
    name : "costContextParking",
    // preferences over coffee places with parking
    decisionProblem: function(w, a) {
      return _.includes(w, a) ?
                (a == 'parking_coffee'    ? 7  : 3 ) :
                                            a == 'nothing' ? 0 : -Infinity;
   }
  }
);

// extended context without parking preferences
var costContextNoParking = extend(
  costContext,
  {
    name : "costContextNoParking",
    // preferences over coffee places without parking
    decisionProblem: function(w, a) {
      return _.includes(w, a) ?
                (a == 'parking_coffee'    ? 3  : 7) :
                                            a == 'nothing' ? 0 : -Infinity;
   }
  }
);
//////////////////////////////////////////////////////////////////////////// 

////////////////////////////////////////////////////////////////////////////
//  -----------
// | Q&A model |
//  -----------
////////////////////////////////////////////////////////////////////////////

///////////////////////// updated policy with by-cost-type differences /////////////////////////
var getCostActionPolicy = function(beliefs, context, params) {
  var actPol = Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    var decisionProblem = context.decisionProblem;
    var EU = expectation(beliefs, function(world) {
      decisionProblem(world,action)
    })
    factor(params.policyAlpha * EU);
    return action; 
  });
  return actPol
};
///////////////////////////////////////////////////////////////////////////////////////////////

// returns TRUE if a response is contradictory in the light of the question
// example: "Is there a coffee place" - "No, there is one with parking around the corner."
var isContradiction = function(context, question, response) {
  var meaning = context.meaning;
  var isContra = all(
    function(world) {
      !meaning(world,question,response)
    },
    context.worlds
  )
  return isContra
}

// gives updated beliefs about world state after hearing response to question
// based on a literal interpretation of the response
var updateBeliefs = function(beliefs, question, response, context) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    condition(meaning(world, question, response));
    return world;
  });
};

// utility of a question is equal to the expected _value_ of the
// DP after receiving a response minus the cost:
// U(Q)
// _value_ depends on DM's action policy
var questionUtility = function(utterance, beliefs, context, params) {
  // questioner wants to *maximize* expected payoff under decision problem
  var decisionProblem = context.decisionProblem;
  var actionPolicy    = getCostActionPolicy(beliefs, context, params);
  var actionUtility   = expectation(actionPolicy, function(action) { 
    // weight possible actions proportional to reward
    return expectation(beliefs, function(world) {
      return decisionProblem(world, action); 
    });
  });
  return actionUtility - cost(utterance, params);
};

// base-level respondent chooses any safe and true answer
// with equal probability;
// by construction, the function getLicensedResponseR0(question)
// is the set of all safe and true answers
var R0 = cache(function(question, context, params) {

  var R0BeliefSupport = context.R0PriorOverWorlds.support();  // this is always a Delta-belief
  var world = R0BeliefSupport[0];
  var getLicensedResponsesR0 = context.getLicensedResponsesR0;
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(getLicensedResponsesR0(question));
    var meaning = context.meaning;
    condition(meaning(world, question, response))
    return response;
  });
});

// dummy R0 with response set of R1
var R1ContextFree = cache(function(question, context, params) {
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    var ownBeliefs = context.R0PriorOverWorlds;
    var otherBeliefs = updateBeliefs(context.questionerBeliefs, question, response, context);
    factor(10 * (-KL(ownBeliefs, otherBeliefs) - cost(response, params)));
    return response;
  });
});

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation of the response (as emitted by R0)
var updateBeliefsPragmatic = function(beliefs, question, response, context, params) {
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R0(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation (as emitted by R0 w/ full answer set of R1)
var updateBeliefsPragmaticR1 = function(beliefs, question, response, context, params) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R1ContextFree(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// Q1 selects a question with prob proportional to the expected
// value of the DP after hearing a response
var Q1 = function(context, params) {
  return Infer({method: 'enumerate'}, function(){
    var question = uniformDraw(context.questions);
    var expectedUtility = expectation(context.questionerBeliefs, function(trueWorld) {
      var possibleResponses = R0(question, extend(context, {
        R0PriorOverWorlds: Delta({v: trueWorld})
      }), params)
      return expectation(possibleResponses, function(response) {
        var currBeliefs = context.questionerBeliefs;
        var updatedBeliefs = updateBeliefsPragmatic(currBeliefs, question, response, context, params);
        return questionUtility(response, updatedBeliefs, context, params);
      });
    });
    var questionCost = question.type == 'no-question' ? 0 : params.questionCost;
    factor(params.questionerAlpha * (expectedUtility - questionCost));
    return question.text;
  });
};

//////////////////////////////////////////////////
// R1 : pragmatic respondent
// ---
// R1's prior beliefs are a distribution over
// different context which differ only wrt
// the questioner's beliefs and/or preferences.
// There are two kinds of R1: a sampler and an
// averager. The former is computationally faster,
// the latter is "normatively correct".

////////////////////////// Cost extensions ///////////////////////////////
// R1 beliefs about questioner preferences in an SUV context
var R1PriorContexts = {
  R1PriorSUVContext_secondaryGoals: {
    context_name: "SUV",
    parking: costContextParking,
    noParking: costContextNoParking,
    distribution: Categorical({vs: ["parking", "noParking"], ps: [0.8, 0.2]})
  },
// R1 beliefs about questioner preferences in an pedestrian context
  R1PriorPedestrianContext_secondaryGoals: {
    context_name: "Pedestrian",
    parking: costContextParking,
    noParking: costContextNoParking,
    distribution: Categorical({vs: ["parking", "noParking"], ps: [0.2, 0.8]})
  },
 // R1 beliefs about questioner preferences in a neutral context
  R1PriorNeutralContext_secondaryGoals: {
    context_name: "Neutral",
    parking: costContextParking,
    noParking: costContextNoParking,
    distribution: Categorical({vs: ["parking", "noParking"], ps: [0.5, 0.5]})
  }
}
/////////////////////////////////////////////////////////////////////////
var R1ContextPosterior = cache(function(context, question, R1PriorContext, params) {
  Infer(
    {method:'enumerate'},
    function() {
      var context_label  = sample(R1PriorContext.distribution);
      var context_sample = extend(
        R1PriorContext[context_label],
        {R1PriorOverWorlds: context.R1PriorOverWorlds});
      var questioner = Q1(context_sample, params);
      factor(questioner.score(question.text));
      return {label: context_label, sample: context_sample, name: context_sample.name}
    }
  )
})


// R1 Averager is full rational reasoner, integrating over all relevant uncertainty
// key assumptions:
// - R1 assumes that their responses will be exhaustified (currently with fixed alpha = 10);
//   this assumption is very important, and also deals with "unawareness": not choosing anything
//   that wasn't mentioned blindly
var R1Averager = cache(function(context, R1PriorContext, question, params) {
  console.log("R1 averager ---------------\nquestion given: ", question);
  console.log("R1 averager ---------------\ncontext given: ", R1PriorContext.context_name);
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  var ownBeliefs = context.R1PriorOverWorlds;
  var contextPosterior = marginalize(R1ContextPosterior(context, question,
                                                        R1PriorContext, params), 'sample');
     
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    var expectedUtility = expectation(
      contextPosterior,
      function(context_sample) {
        var otherBeliefs = updateBeliefsPragmaticR1(context_sample.questionerBeliefs,
                                                    question, response, context_sample, params);
        var decisionProblem = context_sample.decisionProblem;
        var actionPolicy = getCostActionPolicy(otherBeliefs, context, params); 
        var actionUtility = expectation(actionPolicy, function(action) { 
          // weight possible actions proportional to reward
          return expectation(otherBeliefs, function(world) {
            return decisionProblem(world, action); 
          });
        });
        var EU = ((1-params.relevanceBetaR1) * -KL(ownBeliefs, otherBeliefs) +
                  params.relevanceBetaR1 * actionUtility -
                  cost(response,params))
        return (EU)
      }
    )
    factor(params.R1Alpha * expectedUtility);
    return response;
  });
});


//////////////////////////////////////////////////
// output for R script
//////////////////////////////////////////////////

// function to call when using RwebPPL
var makeR = function(){

  console.log("Starting task: ", RInput[0].task, RInput[0].task == "safeAnswererPositive")
  if (RInput[0].task == "cost") {
    // target-same-other setting (1st pilot)

    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }
    var contextLocal = extend(
      costContext,
      {decisionProblem: function(w, a) { // preference for coffee shop read from user's input
        return _.includes(w, a) ?
                (a == 'parking_coffee'    ? RInput[0].utilParkingCoffee : RInput[0].utilNoParkingCoffee) : 
                                           a == 'nothing' ? RInput[0].utilNothing : -Infinity;
       }
      }
    )
    var question = contextLocal.questions[0]; 
    
    
    var R1Prediction = R1Averager(contextLocal, R1PriorContexts[RInput[0].R1Context], question, params)
    var R1PredictionReduced = Infer({
      method: 'enumerate'},
      function() {
        var response = sample(R1Prediction);
        var match = response == 'yes.---' ? true :
            response == 'yes.noParking_coffee' ? true :
            response == 'yes.parking_coffee' ? true :
            response == 'yes.noParking_coffee+parking_coffee' ? true : false    
        condition(match)
        return(response)
      })
    console.log("R1-Averager:")
    terminalViz(R1PredictionReduced,4)

    return(R1PredictionReduced)

  } 
}

//////////////////////////////////////////////////
// inferring continuous preferences
// ///////////////////////////////////////////////

// build a context representation for a given pair of
// payoff parameters
var makeContext = function(RP, LC) {
  extend(
    pieCakeContextMinimal,
    {
      questions: [
        {type: 'polar-disjunct', queried: ['RP'], text: 'RP?'},
        {type: 'polar-disjunct', queried: ['LC'], text: 'LC?'},
        // {type: 'wh', queried: bakedGoodsMinimal, text: 'which?'},
        {type: 'no-question', queried: [], text: 'no-Q'}
      ],
      decisionProblem: function(w, a) {
        return a == 'no-order' ? 0 :
        _.includes(w, a) ? (
          a == 'RP' ? RP  : RP+LC) : -1;
      }
    });
}

var getContInf = function(params){

  var getScore = function(RP, LC) {
    // build context with parameters
    var c =  makeContext(RP, LC);
    // questioner choice & score
    var questioner = Q1(c, params)
    var score = questioner.score('RP?')
    return(score)
  }

  var contInf = Infer(
    {method: "MCMC", samples: 4000},
    function() {
      // RP is the reference level
      var RP = gaussian({mu: 2, sigma: 2});;
      // 'coefficients': deviation from reference level
      var LC = gaussian({mu: 0, sigma: 1});
      factor(getScore(RP,LC))
      var out = {RP: RP, LC: LC};
      return (out)
    }
  )

  return(contInf)

}

var continuousInference = function(params){
  var cI = getContInf(params)
  viz(cI)
  viz(marginalize(cI, "RP"))
  viz(marginalize(cI, "LC"))
}

var consoleOutQ1ContInf = function(RP, LC, params) {
  console.log('question probs with: RP =', RP, 'LC =', LC)
  terminalViz(Q1(makeContext(RP,LC), params))
}


////////////////////////////////////////////////////////////////////////////
// dev corner
////////////////////////////////////////////////////////////////////////////

var consoleOutTSO = function(params) {

  var context = tsoContext;
  console.log('context: \t', context.name);

  var question = context.questions[0];
  console.log('question: \t', question.text)

  var world = 'competitor+sameCat+otherCat1+otherCat2';
  console.log('world: \t\t', world);

  var responsesR0 = context.getLicensedResponsesR0
  console.log('R0 resp. set: \t', responsesR0(question))

  var responseR0 = responsesR0(question)[0]
  console.log('response R0: \t', responseR0)

  var meaning = context.meaning
  console.log('truth R0 r.: \t', meaning(world,question,responseR0));

  var responsesR1 = context.getLicensedResponsesR1

  var responseR1 = responsesR1(question)[0]
  console.log('response R1: \t', responseR1)

  var meaning = context.meaning
  console.log('truth R1 r.: \t', meaning(world,question,responseR1));

  var R0_test = R0(question, context, params)

  console.log("R0:")
  terminalViz(R0_test, 4)

  var Q1_test = Q1(context, params)
  console.log("Q1:")
  terminalViz(Q1_test, 4)

  // var R0Ext_test = R1ContextFree(question, context, params)
  // console.log("R0 (extended):")
  // terminalViz(R0Ext_test, 4)

  // ---------------------------------------------
  // testing R1 with full knowledge of preferences
  // ---------------------------------------------

  // R1 knows full context model
  var R1Prior_FullKnowledge = {
    trueWorld: tsoContext,
    distribution: Categorical({vs: ["trueWorld"]})
  }

  var R1posterior = R1ContextPosterior(tsoContext, question, R1Prior_FullKnowledge, params);
  console.log("Posterior inference (trivial)")
  terminalViz(marginalize(R1posterior, 'label'),4)

  var R1Prediction = R1Averager(tsoContext, R1Prior_FullKnowledge, question, params)
  var R1PredictionReduced = Infer({
    method: 'enumerate'},
    function() {
      var response = sample(R1Prediction);
      var match = response == 'no.---' ? true :
          response == 'no.competitor' ? true :
          response == 'no.competitor+sameCat' ? true :
          response == 'no.otherCat' ? true :
          response == 'no.competitor+sameCat+otherCat' ? true : false
      condition(match)
      return(response)
    })
  console.log("R1-Averager (full knowledge):")
  terminalViz(R1PredictionReduced,4)

}

////////////////////////////////////////////////////////////////////////////
// control center      : main functions to call
// ---
// makeR               : use in connection with 'collect-webppl-results.r'
// consoleOut          : use w/ terminal for dev
// continuousInference : use in webppl.org (plotting) of contin. Inference
// consoleOutQ1Cont    : use w/ terminal for dev Q1 beh. in cont. Inference
// consoleOutTSO       : use w/ terminal for dev of TSO case (1st pilot)
// /////////////////////////////////////////////////////////////////////////

//  -------------------
// | global parameters |
//  -------------------

var paramsGlobal = {
  policyAlpha     : 2.5,    // SM-alpha for action policy
  questionerAlpha : 4,      // SM-alpha for question choice
  R1Alpha         : 3,      // SM-alpha for R1
  relevanceBetaR0 : 0,      // beta=1 for only action-utility
  relevanceBetaR1 : 0.95,   //
  costWeight      : 2.5,
  questionCost    : 0.25    // cost for a question (relative to no question)
};


makeR()
// consoleOut(paramsGlobal)
// continuousInference(paramsGlobal)
// consoleOutQ1ContInf(5,-1, paramsGlobal)

// consoleOutTSO(paramsGlobal); // compare against pilot data from N=50
          // frequency of answer types:
          // 0.1267 0.5698 0.2068 0.0000 0.0967
