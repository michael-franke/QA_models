//  -------------------
// | utility functions |
//  -------------------

// poor person's barplot emulation for terminal
var terminalViz = function(dist, precisionLevel) {

  var unfiltered_support = dist.support();
  var unfiltered_probs   = map(function(x) {
    return 1*dist.score(x).toPrecision(precisionLevel)}, dist.support())

  var unsorted_probs   = filter(function(x) {return Math.exp(x) > 0},
                                unfiltered_probs)
  var unsorted_support = filter(function(x) {return Math.exp(dist.score(x)) > 0},
                                unfiltered_support )

  var sorted_probs     = sort(unsorted_probs);
  var sortFunction = function(x) {
    return -1*dist.score(x).toPrecision(precisionLevel)
  }
  var sorted_support   = sortOn(unsorted_support, sortFunction)
  var max_length_element = _.max(map(function(e) {e.length}, sorted_support));
  var scores = map(function(x) {
    return 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
  }, sorted_support)
  var maxScore =  _.max(map(function(e) {e}, scores));
  map(
    function(x) {
      var score = 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
      console.log(" ",
                  _.padEnd(x, max_length_element, " "),
                  ": ",
                  _.padEnd(_.repeat('*', score*20), 21),
                  score
                 )}
    , sorted_support)
  return "  ===viz==="
}

var butLast = function(xs){
  return xs.slice(0, xs.length-1);
};

var KL = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return (probP == 0.0 ? 0.0 :
            probQ == 0.0 ? 1000000:
            probP * (scoreP - scoreQ));
  }, values));
};

var SumSquares = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return ((scoreP - scoreQ)^2);
  }, values));
};

var powerset = function(set) {
  if (set.length == 0) {
    return [[]];
  } else {
    var rest = powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

// replace empty string with word 'nothing'
var replaceEmptyListWithStringNothing = function(set) {
  _.concat(filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    set
  ), "nothing");
}


//  -----------------------------------------------
// | helper functions for preparing context models |
//  -----------------------------------------------
//
// Context models are built from a set of 'atoms', e.g.,
// the whole set of single entities in the domain.
//
// The function 'prepareContextSets' returns a dictionary
// with:
// * the set of all world states (power set of atoms),
// * the set of all licensed responses for R1 to a WH question
// * the set of all licensed responses for R1 to a polar question
//
// The function 'meaningFunction' is the generic semantic function
// to be used in (non-minimal) cases where more than one element/item
// can be true/present.
// TODO: check if the meaning function for the minimal context models
// can be subsumed under this one.

var prepareContextSets = function(atoms) {

  // The function 'makePowerSet' takes a list of atoms as input and
  // returns an array of strings describing each world state
  // (e.g., which items are present).
  var makePowerSet = function (atoms) {
    // create string representation of all subsets of atoms
    var setWithEmptyListElement = map(
      function(v){return v.join('+');},
      // add possibility to encode background knowledge of number k of available goods
      filter(function(x) {return x.length <= 8}, powerset(atoms))
    );
    return(replaceEmptyListWithStringNothing(setWithEmptyListElement));
  };

  var makeResponsePowerSet = function (atoms) {
    // create string representation of all subsets of atoms
    var setWithEmptyListElement = map(
      function(v){return v.join('+');},
      // add possibility to encode background knowledge of number k of available goods
      filter(function(x) {return x.length <= 8}, powerset(_.flatten(map(function(x){return [x, "not."+x]}, atoms))))
    );
    return(replaceEmptyListWithStringNothing(setWithEmptyListElement));
  };

  ////// TODO: somehow I failed to automatically generate the R1PolarResponses
  ////// which would include yes, no, AE, not_AE etc
  ////// therefore, they are manually specified below for now
  var R1PolarResponses = ['yes.---', 'no.---', 'yes.AE', 'yes.CB', 'yes.not_AE', 'yes.not_CB', 'yes.AE+CB', 'yes.not_AE+CB', 'yes.AE+not_CB'] // assume neither = no.---
  var powerSet = makePowerSet(atoms);
  var responsePowerSet = makeResponsePowerSet(atoms);
  // return powerset, R1 licencesed responses
  var out = {
    'atoms'            : atoms,
    'powerSet'         : powerSet,
    'R1WHResponses'    : powerSet,
    'R1PolarResponses' : R1PolarResponses
  }
  return(out)
}

///////////// PT's extension for prior probability of options - sensitive QA /////////
//// running example: 'do you accept credit cards?', where probability of having / accepting AE >> CB
//// literal semantics below extended to be able to handle negation (e.g., yes, but not AE)
//// assumed for of the possible utterances is yes/no.<atom>, where atom can be AE, not_AE, AE+CB, not_CB+AE etc
//// TODO: check if the representation of negation is correct 
//// TODO: some question types are not supported yet
var meaningFunction = function(world, question, response) {

  // meaning of literals / atoms
  var meaning_atomic = function(world, question, response) {
    if(response == '' || response == "---") {
      // assume silence has null meaning
      return true;
    }
    if(response == "nothing") {
      if(world == "nothing") {
        return true;
      }
    }
    if(world == "nothing" && question.type == 'wh') {
      return response == "nothing";
    }
    // filter out negated atoms / conjuncts
    var neg_conjuncts = filter(function(x){return x.split('_')[0] == 'not';}, response.split('+'));
    // split the negated items into "not" and the item, so that these can be removed from the set of possible
    // items and the meaning of negation can be represented
    var neg_conjuncts_wo_negation = map(function(x){return x.split('_')[1];}, neg_conjuncts);
    // if the world contains the negated atom, the utterance is false, otherwise true
    var x = filter(function(x){return !_.includes(neg_conjuncts_wo_negation, x);}, neg_conjuncts_wo_negation.length > 0 ? ['AE', 'CB'] : response.split('+') );
    // NOTE: this hack part is necessary because if the list of stated options is just empty,
    // the function all() will return true irrespectively of the world
    var available_stated_conjuncts = x.length === 0 ? ['---'] : x;
    
    if(question.type == 'single-item') {
      return (
        response == 'yes' ? _.intersection(world.split('+'), question.queried).length > 0 : 
          response == 'no' ?  _.intersection(world.split('+'), question.queried).length == 0 : 
          all(function(item) { 
            return _.includes(world.split('+'), item);
          }, available_stated_conjuncts)); 
    } else if(question.type == 'wh') {
        // assume response is true when the shop contains every mentioned item
        return all(function(item) {
          return _.includes(world.split('+'), item);
        }, available_stated_conjuncts); 
    } else {
        return console.error('question type not yet supported: ' + question.type);
    }
  }

  // meaning of conjunctions

  return all(
    function (r) {
      meaning_atomic(world, question, r)
    },
    _.split(response, '.')
  )

}


// response cost is proportional to length in words
var cost = function(response,params) {
  return _.includes(response, "---") ? 0 : params.costWeight * (response.split('+').length);
};
//  --------------------------
// | preparing context models |
//  --------------------------

///////////////////////// PTs context extensions for prior-sensitive QA reasoning /////////////////
// the alternatives are constructed by <property>_<item>
// depending on context, the speaker may or may not reason about the property and respective cost
var priorAtoms = ['AE', 'CB']
var priorR1WHResponses = ['AE+CB', 'AE', 'CB']
// construct the set of possible worlds
var priorPowerSet = filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    map(
      function(v){return v.join('+');}, powerset(priorAtoms)
    )
)

// construct possible responses
var priorR1PolarResponses = prepareContextSets(priorAtoms).R1PolarResponses;
// {typ, atypisch, not typ, not atyp, both, typ & not atyp, atyp & not typ, neiter}
var priorContext = {
  name : "priorContext",
  speakerWorlds : priorPowerSet,
  questionerWorlds: priorPowerSet,
  actions: ["AE", "CB", "nothing"], // assume 'nothing' is just doing nothing -- TODO: check if this matches assumption that person chooses to go to restaurant or not
  questions: [ // TODO properly implement all question types 
    {type: 'single-item', queried: ['AE','CB', 'AE+CB'], text: 'Any?'}, 
    {type: 'wh', queried: ['AE','CB'], text: 'Which?'},
    {type: 'single-item', queried: ['AE'], text: 'Typical?'},
    {type: 'single-item', queried: ['CB'], text: 'Atypical?'},
  ],
  // the questioner's prior over the cards that the restaurant (i.e., responded accepts)
  // TODO: this is completely randomly chosen and hard-coded, implement properly
  QPriorOverWorlds: Categorical({vs: priorPowerSet, ps: [0.85, 0.1, 0.05]}), //NOTE: this is different from the respondent's prior
  // the respondent's available cards
  // subject to overriding with input in different contexts
  // TODO: check if this is a correct representation
  R0PriorOverWorlds: Delta({v: 'AE'}), 
  R1PriorOverWorlds: Delta({v: 'AE'}),
  // depends on questioner's own card 
  // decision utility maps triple W_c x W_o x A unto reals, s.t.,
  // U(wc, wo, “go") = 1 if wo and wc have non-empty intersection (0 otherwise)
  // U(wc, wo, “stay") = 1 - U(wc, wo, “go”)
  // TODO: check if this is a correct representation
  decisionProblem: function(w_s, a) { 
      return _.includes(w_s, a) ?
                0.9 :
                (a == "nothing"   ? 0.1 : 0);
  },
  
  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            priorR1WHResponses : priorR1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunction 
};

/////// PT's extension for prior-sensitive QA reasoning /////////
// extended context depending on available cards
// lowProb context is the one where the low prob card is available for the questioner
// while the respondent has the high prob card
var contextLowProb_AE = extend(
    priorContext,
    {
      name : "priorContextLowProb",
      actions: ["CB", "nothing"],
      R0PriorOverWorlds: Delta({v: 'AE'})
    }
)
// here, the respondent had the low prob card, too
var contextLowProb_CB = extend(
  priorContext,
  {
    name : "priorContextLowProb",
    actions: ["CB", "nothing"],
    R0PriorOverWorlds: Delta({v: 'CB'})
  }
)
// highProb context is the one where the high prob card is available for the questioner
// and the respondent has the high prob card, too
var contextHighProb_AE = extend(
  priorContext,
  {
    name : "priorContextHighProb",
    actions: ["AE", "nothing"],
    R0PriorOverWorlds: Delta({v: 'AE'}),
  }
)
// here the respondent had the low prob card
var contextHighProb_CB = extend(
  priorContext,
  {
    name : "priorContextHighProb",
    actions: ["AE", "nothing"],
    R0PriorOverWorlds: Delta({v: 'CB'}),
  }
)

//// PT's extension representing two possible respondent contexts,
//// where she is going to reason about like questioner cards
//// TODO: respondend's prior over the questioner's cards is hard-coded and manually set
var priorContexts = {
  'priorContextLowProb': { // respondent has low prob card
    context_name: "priorContextLowProb",
    typicalQ: contextHighProb_CB, // low prob state of the world where Q has high prob preferences
    atypicalQ: contextLowProb_CB, // low prob state of the world where Q has low prob preferences
    distribution: Categorical({vs: ["typicalQ", "atypicalQ"], ps: [0.9, 0.1]})
  },
  'priorContextHighProb': { // respondent has high prob card
    context_name: "priorContextHighProb",
    typicalQ: contextHighProb_AE, // high prob state of the world where Q has high prob preferences
    atypicalQ: contextLowProb_AE, // high prob state of the world where Q has low prob preferences
    distribution: Categorical({vs: ["typicalQ", "atypicalQ"], ps: [0.9, 0.1]})
  },
}
//////////////////////////////////////////////////////////////////////////// 

////////////////////////////////////////////////////////////////////////////
//  -----------
// | Q&A model |
//  -----------
////////////////////////////////////////////////////////////////////////////

var getPriorActionPolicy = function(beliefs, context, params) {
  var actPol = Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    // console.log("getPriorActionPolicy ", action)
    // draw type of context
    //var type = uniformDraw(context.type);
    var decisionProblem = context.decisionProblem;
    var EU = expectation(beliefs, function(world) {
      decisionProblem(world, action)
    })
    factor(params.policyAlpha * EU);
    return action; 
  });
  return actPol
};
///////////////////////////////////////////////////////////////////////////////////////////////

// returns TRUE if a response is contradictory in the light of the question
// example: "Do you have any pie?" - "No, we have lemon pie."
var isContradiction = function(context, question, response) {
  // console.log("executing is Contra")
  var meaning = context.meaning;
  var isContra = all(
    function(world) {
      console.log("meaning ", world, question, response, !meaning(world,question,response))
      !meaning(world,question,response)
    },
    context.speakerWorlds
  )
  // console.log('is Contra ', isContra)
  return isContra
}

// gives updated beliefs about world state after hearing response to question
// based on a literal interpretation of the response
var updateBeliefs = function(beliefs, question, response, context) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    condition(meaning(world, question, response));
    return world;
  });
};

// utility of a question is equal to the expected _value_ of the
// DP after receiving a response minus the cost:
// U(Q)
// _value_ depends on DM's action policy
var questionUtility = function(utterance, beliefs, context, params) {
  // questioner wants to *maximize* expected payoff under decision problem
  var decisionProblem = context.decisionProblem;
  var actionPolicy    = getPriorActionPolicy(beliefs, context, params);
  var actionUtility   = expectation(actionPolicy, function(action) { // function(action)
    // weight possible actions proportional to reward
    return expectation(beliefs, function(world) {
      return decisionProblem(world, action); // decisionProblem(world, action);
    });
  });
  return actionUtility - cost(utterance, params);
};


// base-level respondent chooses any safe and true answer
// with equal probability;
// by construction, the function getLicensedResponseR0(question)
// is the set of all safe and true answers
var R0 = cache(function(question, context, params) {

  var R0BeliefSupport = context.R0PriorOverWorlds.support();  // this is always a Delta-belief
  var world = R0BeliefSupport[0];
  console.log("R0 world ", world)
  var getLicensedResponsesR0 = context.getLicensedResponsesR0;
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(getLicensedResponsesR0(question));
    var meaning = context.meaning;
    condition(meaning(world, question, response))
    return response;
  });
});

// dummy R0 with response set of R1
var R1ContextFree = cache(function(question, context, params) {
  console.log('--------R1ContextFree-----------')
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    // console.log('R1Context free response sample', response);
    var ownBeliefs = context.R0PriorOverWorlds;
    // console.log('own beliefs: ', ownBeliefs);
    var otherBeliefs = updateBeliefs(context.QPriorOverWorlds, question, response, context);
    factor(10 * (-KL(ownBeliefs, otherBeliefs) - cost(response, params)));
    return response;
  });
});


// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation of the response (as emitted by R0)
// TODO check if type needs to be included here as well?
var updateBeliefsPragmatic = function(beliefs, question, response, context, params) {
  // var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R0(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation (as emitted by R0 w/ full answer set of R1)
var updateBeliefsPragmaticR1 = function(beliefs, question, response, context, params) {
  // var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R1ContextFree(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// Q1 selects a question with prob proportional to the expected
// value of the DP after hearing a response
var Q1 = function(context, params) {
  return Infer({method: 'enumerate'}, function(){
    var question = uniformDraw(context.questions);
    //console.log('considering question', question.queried);
    var expectedUtility = expectation(context.QPriorOverWorlds, function(trueWorld) {
      //console.log('Q1 in possible world', trueWorld);
      var possibleResponses = R0(question, extend(context, {
        R0PriorOverWorlds: Delta({v: trueWorld})
      }), params)
      //console.log('respondent: ', possibleResponses)
      return expectation(possibleResponses, function(response) {
        //console.log('considering response ', response );
        var currBeliefs = context.QPriorOverWorlds;
       // console.log('currBeliefs ', currBeliefs);
        var updatedBeliefs = updateBeliefsPragmatic(currBeliefs, question, response, context, params);
        return questionUtility(response, updatedBeliefs, context, params);
      });
    });
    //     console.log('expected utility: ',expectedUtility)
    var questionCost = question.type == 'no-question' ? 0 : params.questionCost;
    factor(params.questionerAlpha * (expectedUtility - questionCost));
    return question.text;
  });
};

//////////////////////////////////////////////////
// R1 : pragmatic respondent
// ---
// R1's prior beliefs are a distribution over
// different context which differ only wrt
// the questioner's beliefs and/or preferences.
// There are two kinds of R1: a sampler and an
// averager. The former is computationally faster,
// the latter is "normatively correct".
//////////////////////////////////////////////////
var R1ContextPosterior = cache(function(context, question, R1PriorContext, params) {
  Infer(
    {method:'enumerate'},
    function() {
      var context_label  = sample(R1PriorContext.distribution);
      var context_sample = extend(
        R1PriorContext[context_label],
        {R1PriorOverWorlds: context.R1PriorOverWorlds});
        // console.log("---- R1 context posterior context sample ", context_sample)
      // var questioner_preference = sample(context.RPriorOverPreference);
      var questioner = Q1(context_sample, params);
      factor(questioner.score(question.text));
      return {label: context_label, sample: context_sample, name: context_sample.name}
    }
  )
})


// R1 Averager is full rational reasoner, integrating over all relevant uncertainty
// key assumptions:
// - R1 assumes that their responses will be exhaustified (currently with fixed alpha = 10);
//   this assumption is very important, and also deals with "unawareness": not choosing anything
//   that wasn't mentioned blindly
var R1Averager = cache(function(context, R1PriorContext, question, params) {
  // console.log("R1 averager ---------------\nquestion given: ", question);
  // console.log("R1 averager context: ", context);
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  // console.log(getLicensedResponsesR1(question));
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  // console.log("available responses R1 Averager: ", responses);
  var ownBeliefs = context.R1PriorOverWorlds;
  // console.log('ownBeliefs R1Averager ', ownBeliefs)

  var contextPosterior = marginalize(R1ContextPosterior(context, question,
                                                        R1PriorContext, params), 'sample');

  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    var expectedUtility = expectation(
      contextPosterior,
      function(context_sample) {
        var otherBeliefs = updateBeliefsPragmaticR1(context_sample.QPriorOverWorlds,
                                                    question, response, context_sample, params);
         terminalViz(otherBeliefs, 4)
        var decisionProblem = context_sample.decisionProblem;
        var actionPolicy = getPriorActionPolicy(otherBeliefs, context, params); 
        var actionUtility = expectation(actionPolicy, function(action) { 
          // weight possible actions proportional to reward
          return expectation(otherBeliefs, function(world) {
            return decisionProblem(world, action); 
          });
        });
        var EU = ((1-params.relevanceBetaR1) * -KL(ownBeliefs, otherBeliefs) +
                  params.relevanceBetaR1 * actionUtility -
                  cost(response,params))
        return (EU)
      }
    )
    factor(params.R1Alpha * expectedUtility);
    return response;
  });
});

//////////////////////////////////////////////////
// output for R script
//////////////////////////////////////////////////

// function to call when using RwebPPL
var makeR = function(){

  console.log("Starting task: ", RInput[0].task, RInput[0].task == "safeAnswererPositive")

    if (RInput[0].task == "prior") {
    // target-same-other setting (1st pilot)

    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   //
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }

    var contextLocal = extend(
      priorContext,
      // TODO: check if speakerWorlds is used correctly and consistently with the cards available to respondent in given context
      {speakerWorlds: [RInput[0].R0PriorOverWorlds],
        R0PriorOverWorlds: Delta({v: RInput[0].R0PriorOverWorlds}),
      R1PriorOverWorlds: Delta({v: RInput[0].R1PriorOverWorlds})}
    )
    var question = contextLocal.questions[0]; 
    
    // NOTE: contextLocal passed here as first arg encodes typical Q preference 
    var R1Prediction = R1Averager(contextLocal, priorContexts[RInput[0].R1Context], question, params)
    // console.log("R1Prediction: ", R1Prediction)
    var R1PredictionReduced = Infer({
      method: 'enumerate'},
      function() {
        var response = sample(R1Prediction);
        var match = response == 'yes.---' ? true :
            response == 'yes.AE+CB' ? true :
            response == 'yes.AE' ? true :
            response == 'yes.CB' ? true :
            response == 'yes.AE+not_CB' ? true :
            response == 'yes.not_AE+CB' ? true :
            response == 'yes.CB+not_AE' ? true :
            response == 'yes.not_AE' ? true :
            response == 'yes.not_CB' ? true : false    
        condition(match)
        return(response)
      })
    console.log("R1-Averager:")
    terminalViz(R1PredictionReduced,4)

    return(R1PredictionReduced)
  }  else {
    console.log("Task not recognized")
  }
}


////////////////////////////////////////////////////////////////////////////
// control center      : main functions to call
// ---
// makeR               : use in connection with 'collect-webppl-results.r'
// consoleOut          : use w/ terminal for dev
// continuousInference : use in webppl.org (plotting) of contin. Inference
// consoleOutQ1Cont    : use w/ terminal for dev Q1 beh. in cont. Inference
// consoleOutTSO       : use w/ terminal for dev of TSO case (1st pilot)
// /////////////////////////////////////////////////////////////////////////

//  -------------------
// | global parameters |
//  -------------------

var paramsGlobal = {
  policyAlpha     : 2.5,    // SM-alpha for action policy
  questionerAlpha : 4,      // SM-alpha for question choice
  R1Alpha         : 3,      // SM-alpha for R1
  relevanceBetaR0 : 0,      // beta=1 for only action-utility
  relevanceBetaR1 : 0.95,   //
  costWeight      : 2.5,
  questionCost    : 0.25    // cost for a question (relative to no question)
};


makeR()