//  -------------------
// | utility functions |
//  -------------------

// poor person's barplot emulation for terminal
var terminalViz = function(dist, precisionLevel) {

  var unfiltered_support = dist.support();
  var unfiltered_probs   = map(function(x) {
    return 1*dist.score(x).toPrecision(precisionLevel)}, dist.support())

  var unsorted_probs   = filter(function(x) {return Math.exp(x) > 0},
                                unfiltered_probs)
  var unsorted_support = filter(function(x) {return Math.exp(dist.score(x)) > 0},
                                unfiltered_support )

  var sorted_probs     = sort(unsorted_probs);
  var sortFunction = function(x) {
    return -1*dist.score(x).toPrecision(precisionLevel)
  }
  var sorted_support   = sortOn(unsorted_support, sortFunction)
  var max_length_element = _.max(map(function(e) {e.length}, sorted_support));
  var scores = map(function(x) {
    return 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
  }, sorted_support)
  var maxScore =  _.max(map(function(e) {e}, scores));
  map(
    function(x) {
      var score = 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
      console.log(" ",
                  _.padEnd(x, max_length_element, " "),
                  ": ",
                  _.padEnd(_.repeat('*', score*20), 21),
                  score
                 )}
    , sorted_support)
  return "  ===viz==="
}

var butLast = function(xs){
  return xs.slice(0, xs.length-1);
};

var KL = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return (probP == 0.0 ? 0.0 :
            probQ == 0.0 ? 1000000:
            probP * (scoreP - scoreQ));
  }, values));
};

var SumSquares = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return ((scoreP - scoreQ)^2);
  }, values));
};

var powerset = function(set) {
  if (set.length == 0) {
    return [[]];
  } else {
    var rest = powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

// replace empty string with word 'nothing'
var replaceEmptyListWithStringNothing = function(set) {
  _.concat(filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    set
  ), "nothing");
}


//  -----------------------------------------------
// | helper functions for preparing context models |
//  -----------------------------------------------
//
// Context models are built from a set of 'atoms', e.g.,
// the whole set of single entities in the domain.
//
// The function 'prepareContextSets' returns a dictionary
// with:
// * the set of all world states (power set of atoms),
// * the set of all licensed responses for R1 to a WH question
// * the set of all licensed responses for R1 to a polar question
//
// The function 'meaningFunction' is the generic semantic function
// to be used in (non-minimal) cases where more than one element/item
// can be true/present.
// TODO: check if the meaning function for the minimal context models
// can be subsumed under this one.

var prepareContextSets = function(atoms) {

  // The function 'makePowerSet' takes a list of atoms as input and
  // returns an array of strings describing each world state
  // (e.g., which items are present).
  var makePowerSet = function (atoms) {
    // create string representation of all subsets of atoms
    var setWithEmptyListElement = map(
      function(v){return v.join('+');},
      // add possibility to encode background knowledge of number k of available goods
      filter(function(x) {return x.length <= 8}, powerset(atoms))
    );
    return(replaceEmptyListWithStringNothing(setWithEmptyListElement));
  };

  // The function 'makeR1polarResponses' takes the atoms and the power set and
  // creates all licensed R1 responses to a polar question.
  var makeR1polarResponses = function(atoms, powerSet) {

    var sampleR1PolarResponses = Infer(
      {method: 'enumerate'},
      function() {
        var yesNoPart = uniformDraw(["yes", "no"]);
        var itemPart = uniformDraw(_.concat(filter(
          function(x) {
            if (x != "") {
              return x
            }
          },
          powerSet
        ), "---", "nothing"));
        return [yesNoPart, itemPart].join(".")
      }
    )

    // exclude utterance 'no, we have ... exhaustive list of everything'
    // and 'yes, we have nothing'
    var x = map(function(a) {a == atoms[0] ? a : a + "+" }, atoms.reverse())
    var contradiction = reduce(function(a, acc) { acc + a }, "no.", x)

    var R1PolarResponses = filter(
      function(r) {
        // some responses can never be true
        r != "yes.nothing" && r != contradiction
      },
      sampleR1PolarResponses.support());
    return(R1PolarResponses)
  }

  var powerSet = makePowerSet(atoms);

  // return powerset, R1 licencesed responses
  var out = {
    'atoms'            : atoms,
    'powerSet'         : powerSet,
    'R1WHResponses'    : powerSet,
    'R1PolarResponses' : makeR1polarResponses(atoms, powerSet)
  }
  return(out)
}


var meaningFunction = function(world, question, response) {

  // meaning of literals / atoms
  var meaning_atomic = function(world, question, response) {
    // console.log(" *** now evaluating response: part ", response)
    if(response == '' || response == "---") {
      // assume silence has null meaning
      return true;
    }
    if(response == "nothing") {
      if(world == "nothing") {
        return true;
      }
    }
    if(world == "nothing" && question.type == 'wh') {
      return response == "nothing";
    }
    if(question.type == 'single-item') {
      return (
        response == 'yes' ? _.intersection(world.split('+'), question.queried).length > 0 : 
          response == 'no' ?  _.intersection(world.split('+'), question.queried).length == 0 : 
          all(function(item) { 
            return _.includes(world.split('+'), item);
          }, response.split('+')));
    } else if(question.type == 'wh') {
        // assume response is true when the shop contains every mentioned item
        return all(function(item) {
          return _.includes(world.split('+'), item);
        }, response.split('+'));
      } else {
        return console.error('question type not yet supported: ' + question.type);
      }
  }

  // meaning of conjunctions

  return all(
    function (r) {
      meaning_atomic(world, question, r)
    },
    _.split(response, '.')
  )

}


// response cost is proportional to length in words
var cost = function(response,params) {
  return _.includes(response, "---") ? 0 : params.costWeight * (response.split('+').length);
};
//  --------------------------
// | preparing context models |
//  --------------------------

///////////////////////// PTs context extensions for prior-sensitive QA reasoning /////////////////
// the alternatives are constructed by <property>_<item>
// depending on context, the speaker may or may not reason about the property and respective cost
var priorAtoms = ['AE', 'MC', 'V', 'CB']
var priorR1WHResponses = ['AE+MC+V+CB']
// construct the set of possible worlds
var priorPowerSet = filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    map(
      function(v){return v.join('+');}, powerset(priorAtoms)
    )
)
// construct possible responses
var priorR1PolarResponses = prepareContextSets(priorAtoms).R1PolarResponses;
// helper for constructing subset probs
var getSubsetProbs = function(powerset, probs) {
  var powersetProbsList = map(function(x){
    return sum(map(function(j){return probs[j]}, x.split("+")))
  }, powerset);
  return powersetProbsList
}

var preferencePrior = {
    "AE": 0.55,
    "MC": 0.25,
    "V": 0.15,
    "CB": 0.05
}
var priorContext = {
  name : "priorContext",
  worlds : priorPowerSet,
  actions: _.concat(priorAtoms, "nothing"), // assume 'nothing' is just doing nothing -- TODO: check if this matches assumption that person chooses to go to restaurant or not
  questions: [
    {type: 'single-item', queried: ['AE','MC','V','CB'], text: 'Accept credit cards?'} // TODO: potentially extend with asking for specific types of cards
  ],
  // assume that the probability of occurrence = acceptance of the cards is AE: 0.55, MC: 0.25, V: 0.15, CB: 0.05
  // assume that acceptance of different cards is disjoint, so unnormalized P(["AE","MC","V","CB"]) = 0.55+0.25+0.15+0.05
  questionerBeliefs: Categorical({vs: priorPowerSet, ps: getSubsetProbs(priorPowerSet, preferencePrior)}),
  // R's prior over questioner's card
  RPriorOverPreference: Categorical({vs: priorAtoms, ps: [0.55, 0.25, 0.15, 0.05]}),
  R0PriorOverWorlds: Delta({v: 'AE+MC+V'}), // subject to change in different contexts
  R1PriorOverWorlds: Delta({v: 'AE+MC+V'}),
  decisionProblem: function(w, a, c) { // depends on questioner's own card which is passed as c
      return _.includes(w, a) ?
                (a == c    ? 7  : 
                             1) :
                             0.0001;
  },

  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            priorR1WHResponses : priorR1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunction 
};

// extended context depending on available cards
var priorContextLowProb = extend(
    priorContext,
    {
      name : "priorContextLowProb",
      R0PriorOverWorlds: Delta({v: 'AE+MC+CB'}),
      R1PriorOverWorlds: Delta({v: 'AE+MC+CB'}),
    }
)

// extended context with parking preferences
var priorContextHighProb = extend(
    priorContext,
    {
      name : "priorContextHighProb",
      R0PriorOverWorlds: Delta({v: 'AE+MC+V'}),
      R1PriorOverWorlds: Delta({v: 'AE+MC+V'}),
    }
  )
  
var priorContexts = {
  'priorContextLowProb': priorContextLowProb,
  'priorContextHighProb': priorContextHighProb
}
//////////////////////////////////////////////////////////////////////////// 

////////////////////////////////////////////////////////////////////////////
//  -----------
// | Q&A model |
//  -----------
////////////////////////////////////////////////////////////////////////////

///////////////////////// updated policy with by-cost-type differences /////////////////////////
var getPriorActionPolicy = function(beliefs, context, type, params) {
  var actPol = Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    // draw type of context
    //var type = uniformDraw(context.type);
    var decisionProblem = context.decisionProblem;
    var EU = expectation(beliefs, function(world) {
      decisionProblem(world,action,type)
    })
    factor(params.policyAlpha * EU);
    return action; //{action: action, type: type};
  });
  return actPol
};
///////////////////////////////////////////////////////////////////////////////////////////////

// returns TRUE if a response is contradictory in the light of the question
// example: "Do you have any pie?" - "No, we have lemon pie."
var isContradiction = function(context, question, response) {
  var meaning = context.meaning;
  var isContra = all(
    function(world) {
      !meaning(world,question,response)
    },
    context.worlds
  )
  return isContra
}

// gives updated beliefs about world state after hearing response to question
// based on a literal interpretation of the response
var updateBeliefs = function(beliefs, question, response, context) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    condition(meaning(world, question, response));
    return world;
  });
};

// utility of a question is equal to the expected _value_ of the
// DP after receiving a response minus the cost:
// U(Q)
// _value_ depends on DM's action policy
var questionUtility = function(utterance, beliefs, context, type, params) {
  // questioner wants to *maximize* expected payoff under decision problem
  var decisionProblem = context.decisionProblem;
  var actionPolicy    = getPriorActionPolicy(beliefs, context, type, params);
  var actionUtility   = expectation(actionPolicy, function(action,type) { // function(action)
    // weight possible actions proportional to reward
    return expectation(beliefs, function(world) {
      return decisionProblem(world, action, type); // decisionProblem(world, action);
    });
  });
  return actionUtility - cost(utterance, params);
};


// base-level respondent chooses any safe and true answer
// with equal probability;
// by construction, the function getLicensedResponseR0(question)
// is the set of all safe and true answers
var R0 = cache(function(question, context, params) {
//console.log('--------R0-----------', context)

  var R0BeliefSupport = context.R0PriorOverWorlds.support();  // this is always a Delta-belief
  //console.log(context.R0PriorOverWorlds);
  //console.log('R0BeliefSupport ', R0BeliefSupport)
  var world = R0BeliefSupport[0];
  var getLicensedResponsesR0 = context.getLicensedResponsesR0;
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(getLicensedResponsesR0(question));
    var meaning = context.meaning;
    condition(meaning(world, question, response))
    return response;
  });
});

// dummy R0 with response set of R1
var R1ContextFree = cache(function(question, context, params) {
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  // console.log("R1 dummy, licensed responses: ", responses)
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    // console.log(response);
    var ownBeliefs = context.R0PriorOverWorlds;
    var otherBeliefs = updateBeliefs(context.questionerBeliefs, question, response, context);
    // console.log('updated beliefs: ');
    // terminalViz(otherBeliefs, 4);
    factor(10 * (-KL(ownBeliefs, otherBeliefs) - cost(response, params)));
    return response;
  });
});

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation of the response (as emitted by R0)
// TODO check if type needs to be included here as well?
var updateBeliefsPragmatic = function(beliefs, question, response, context, params) {
  // var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R0(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation (as emitted by R0 w/ full answer set of R1)
var updateBeliefsPragmaticR1 = function(beliefs, question, response, context, params) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R1ContextFree(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// Q1 selects a question with prob proportional to the expected
// value of the DP after hearing a response
var Q1 = function(context, type, params) {
console.log('----------- Q1------------');
console.log('Q1 params ', params)
  return Infer({method: 'enumerate'}, function(){
    var question = uniformDraw(context.questions);
    //console.log('considering question', question.queried);
    var expectedUtility = expectation(context.questionerBeliefs, function(trueWorld) {
      //console.log('Q1 in possible world', trueWorld);
      var possibleResponses = R0(question, extend(context, {
        R0PriorOverWorlds: context.R0PriorOverWorlds //Delta({v: trueWorld})
      }), params)
      //console.log('respondent: ', possibleResponses)
      return expectation(possibleResponses, function(response) {
        //console.log('considering response ', response );
        var currBeliefs = context.questionerBeliefs;
       // console.log('currBeliefs ', currBeliefs);
        var updatedBeliefs = updateBeliefsPragmatic(currBeliefs, question, response, context, params);
        return questionUtility(response, updatedBeliefs, context, type, params);
      });
    });
    //     console.log('expected utility: ',expectedUtility)
    var questionCost = question.type == 'no-question' ? 0 : params.questionCost;
    factor(params.questionerAlpha * (expectedUtility - questionCost));
    return question.text;
  });
};

//////////////////////////////////////////////////
// R1 : pragmatic respondent
// ---
// R1's prior beliefs are a distribution over
// different context which differ only wrt
// the questioner's beliefs and/or preferences.
// There are two kinds of R1: a sampler and an
// averager. The former is computationally faster,
// the latter is "normatively correct".
//////////////////////////////////////////////////
var R1ContextPosterior = cache(function(context, question, R1PriorContext, params) {
  Infer(
    {method:'enumerate'},
    function() {
      var context_label  = sample(R1PriorContext.distribution);
      var context_sample = extend(
        R1PriorContext[context_label],
        {R1PriorOverWorlds: context.R1PriorOverWorlds});
      var questioner_preference = sample(context.RPriorOverPreference);
      var questioner = Q1(context_sample, questioner_preference, params);
      factor(questioner.score(question.text));
      return {label: context_label, sample: context_sample, name: context_sample.name, questioner_preference: questioner_preference}
    }
  )
})

// R1 Averager is full rational reasoner, integrating over all relevant uncertainty
// key assumptions:
// - R1 assumes that their responses will be exhaustified (currently with fixed alpha = 10);
//   this assumption is very important, and also deals with "unawareness": not choosing anything
//   that wasn't mentioned blindly
var R1Averager = cache(function(context, R1PriorContext, question, params) {
  console.log("R1 averager ---------------\nquestion given: ", question);
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  console.log(getLicensedResponsesR1(question));
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  console.log("available responses R1 Averager: ", responses);
  var ownBeliefs = context.R1PriorOverWorlds;
  console.log('ownBeliefs R1Averager ', ownBeliefs)
  var contextPosterior = marginalize(R1ContextPosterior(context, question,
                                                        R1PriorContext, params), 'sample');
  console.log("R1 Averager contextPosterior ", terminalViz(contextPosterior));                                                     
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
     // console.log("\nconsidering response: ", response);
    var expectedUtility = expectation(
      contextPosterior,
      function(context_sample, questioner_preference) {
        //console.log("Context sample: ", context_sample);
        var otherBeliefs = updateBeliefsPragmaticR1(context_sample.questionerBeliefs, // check if updateBeliefsPragmaticR1 alsdo need to update beliefs about the preference type of the questioner
                                                    question, response, context_sample, params);
         // console.log("updated beliefs after response: ", response)
         // terminalViz(otherBeliefs, 4)
        var decisionProblem = context_sample.decisionProblem;
        var actionPolicy = getPriorActionPolicy(otherBeliefs, context, questioner_preference, params); 
        var actionUtility = expectation(actionPolicy, function(action) { // function(action)
          // weight possible actions proportional to reward
          return expectation(otherBeliefs, function(world) {
            return decisionProblem(world, action, questioner_preference); // decisionProblem(world, action);
          });
        });
        var EU = ((1-params.relevanceBetaR1) * -KL(ownBeliefs, otherBeliefs) +
                  params.relevanceBetaR1 * actionUtility -
                  cost(response,params))
        return (EU)
      }
    )
    factor(params.R1Alpha * expectedUtility);
    return response;
  });
});

//////////////////////////////////////////////////
// output for R script
//////////////////////////////////////////////////

// function to call when using RwebPPL
var makeR = function(){

  console.log("Starting task: ", RInput[0].task, RInput[0].task == "safeAnswererPositive")

    if (RInput[0].task == "prior") {
    // target-same-other setting (1st pilot)

    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   //
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }

    console.log('before: ', paramsGlobal)
    console.log('after: ', params)
    var contextLocal = priorContexts[RInput[0].R1Context]
   // console.log(map(function(x){return parseFloat(x)}, RInput[0].RPriorOverPreference.split(',')))
    //var contextLocal = extend(
    //  priorContexts[RInput[0].R1Context],
  //    {RPriorOverPreference: Categorical({vs: priorAtoms, ps: map(function(x){return parseFloat(x)}, RInput[0].RPriorOverPreference.split(','))}),}
  //  )
    console.log('local context: ', contextLocal)
    var question = contextLocal.questions[0]; 
    console.log('question ', question)
    
    // R1 knows full context model
    var R1Prior = {
      trueWorld: contextLocal,
      distribution: Categorical({vs: ["trueWorld"]})
    }
    //console.log("R1Prior: ", priorContextLowProb)

    var R1Prediction = R1Averager(contextLocal, R1Prior, question, params)
    console.log("R1Prediction: ", R1Prediction)
    var R1PredictionReduced = Infer({
      method: 'enumerate'},
      function() {
        var response = sample(R1Prediction);
        var match = response == 'yes.---' ? true :
            response == 'yes.AE+MC+V+CB' ? true :
            response == 'yes.AE+MC' ? true :
            response == 'yes.AE+V+CB' ? true :
            response == 'yes.AE+V' ? true :
            response == 'yes.AE+CB' ? true :
            response == 'yes.AE' ? true :
            response == 'yes.MC+V+CB' ? true :
            response == 'yes.MC+V' ? true :
            response == 'yes.MC+CB' ? true :
            response == 'yes.MC' ? true :
            response == 'yes.V+CB' ? true :
            response == 'yes.V' ? true :
            response == 'yes.CB' ? true :
            response == 'yes.AE+MC+V' ? true :
            response == 'yes.AE+MC+CB' ? true : false    
        condition(match)
        return(response)
      })
    console.log("R1-Averager:")
    terminalViz(R1PredictionReduced,4)

    return(R1PredictionReduced)
  }  else {
    var context =
    RInput[0].task == 'pieCakeContextMinimal' ? pieCakeContextMinimal :
    RInput[0].task == 'pieCakeContextMinimalWithPreferences' ? pieCakeContextMinimalWithPreferences :
    RInput[0].task == 'pieCakeContext' ? pieCakeContext :
    RInput[0].task == 'pieCakeContextAdditivePreferences' ? pieCakeContextAdditivePreferences :
    RInput[0].task == 'pieCakeContextBiasedNoPref' ? pieCakeContextBiasedNoPref :
    RInput[0].task == 'pieCakeContextUnbiasedNoPref' ? pieCakeContextUnbiasedNoPref :
    RInput[0].task == 'pieCakeContextBiasedPessimist' ? pieCakeContextBiasedPessimist :
        false
    Q1(context, params)
  }
}


////////////////////////////////////////////////////////////////////////////
// control center      : main functions to call
// ---
// makeR               : use in connection with 'collect-webppl-results.r'
// consoleOut          : use w/ terminal for dev
// continuousInference : use in webppl.org (plotting) of contin. Inference
// consoleOutQ1Cont    : use w/ terminal for dev Q1 beh. in cont. Inference
// consoleOutTSO       : use w/ terminal for dev of TSO case (1st pilot)
// /////////////////////////////////////////////////////////////////////////

//  -------------------
// | global parameters |
//  -------------------

var paramsGlobal = {
  policyAlpha     : 2.5,    // SM-alpha for action policy
  questionerAlpha : 4,      // SM-alpha for question choice
  R1Alpha         : 3,      // SM-alpha for R1
  relevanceBetaR0 : 0,      // beta=1 for only action-utility
  relevanceBetaR1 : 0.95,   //
  costWeight      : 2.5,
  questionCost    : 0.25    // cost for a question (relative to no question)
};


makeR()
// consoleOut(paramsGlobal)
// continuousInference(paramsGlobal)
// consoleOutQ1ContInf(5,-1, paramsGlobal)

// consoleOutTSO(paramsGlobal); // compare against pilot data from N=50
          // frequency of answer types:
          // 0.1267 0.5698 0.2068 0.0000 0.0967
