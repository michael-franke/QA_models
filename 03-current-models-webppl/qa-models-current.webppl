//  -------------------
// | utility functions |
//  -------------------

// poor person's barplot emulation for terminal
var terminalViz = function(dist, precisionLevel) {

  var unfiltered_support = dist.support();
  var unfiltered_probs   = map(function(x) {
    return 1*dist.score(x).toPrecision(precisionLevel)}, dist.support())

  var unsorted_probs   = filter(function(x) {return Math.exp(x) > 0},
                                unfiltered_probs)
  var unsorted_support = filter(function(x) {return Math.exp(dist.score(x)) > 0},
                                unfiltered_support )

  var sorted_probs     = sort(unsorted_probs);
  var sortFunction = function(x) {
    return -1*dist.score(x).toPrecision(precisionLevel)
  }
  var sorted_support   = sortOn(unsorted_support, sortFunction)
  var max_length_element = _.max(map(function(e) {e.length}, sorted_support));
  var scores = map(function(x) {
    return 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
  }, sorted_support)
  var maxScore =  _.max(map(function(e) {e}, scores));
  map(
    function(x) {
      var score = 1*Math.exp(dist.score(x).toPrecision(precisionLevel)).toPrecision(precisionLevel)
      console.log(" ",
                  _.padEnd(x, max_length_element, " "),
                  ": ",
                  _.padEnd(_.repeat('*', score*20), 21),
                  score
                 )}
    , sorted_support)
  return "  ===viz==="
}

var butLast = function(xs){
  return xs.slice(0, xs.length-1);
};

var KL = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return (probP == 0.0 ? 0.0 :
            probQ == 0.0 ? 1000000:
            probP * (scoreP - scoreQ));
  }, values));
};

var SumSquares = function(dist1, dist2){
  var values = dist1.support();
  return sum(map(function(value){
    var scoreP = dist1.score(value);
    var scoreQ = dist2.score(value);
    var probP = Math.exp(scoreP);
    var probQ = Math.exp(scoreQ);
    return ((scoreP - scoreQ)^2);
  }, values));
};

var powerset = function(set) {
  if (set.length == 0) {
    return [[]];
  } else {
    var rest = powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

// replace empty string with word 'nothing'
var replaceEmptyListWithStringNothing = function(set) {
  _.concat(filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    set
  ), "nothing");
}


//  -----------------------------------------------
// | helper functions for preparing context models |
//  -----------------------------------------------
//
// Context models are built from a set of 'atoms', e.g.,
// the whole set of single entities in the domain.
//
// The function 'prepareContextSets' returns a dictionary
// with:
// * the set of all world states (power set of atoms),
// * the set of all licensed responses for R1 to a WH question
// * the set of all licensed responses for R1 to a polar question
//
// The function 'meaningFunction' is the generic semantic function
// to be used in (non-minimal) cases where more than one element/item
// can be true/present.
// TODO: check if the meaning function for the minimal context models
// can be subsumed under this one.

var prepareContextSets = function(atoms) {

  // The function 'makePowerSet' takes a list of atoms as input and
  // returns an array of strings describing each world state
  // (e.g., which items are present).
  var makePowerSet = function (atoms) {
    // create string representation of all subsets of atoms
    var setWithEmptyListElement = map(
      function(v){return v.join('+');},
      // add possibility to encode background knowledge of number k of available goods
      filter(function(x) {return x.length <= 8}, powerset(atoms))
    );
    return(replaceEmptyListWithStringNothing(setWithEmptyListElement));
  };

  // The function 'makeR1polarResponses' takes the atoms and the power set and
  // creates all licensed R1 responses to a polar question.
  var makeR1polarResponses = function(atoms, powerSet) {

    var sampleR1PolarResponses = Infer(
      {method: 'enumerate'},
      function() {
        var yesNoPart = uniformDraw(["yes", "no"]);
        var itemPart = uniformDraw(_.concat(filter(
          function(x) {
            if (x != "") {
              return x
            }
          },
          powerSet
        ), "---", "nothing"));
        return [yesNoPart, itemPart].join(".")
      }
    )

    // exclude utterance 'no, we have ... exhaustive list of everything'
    // and 'yes, we have nothing'
    var x = map(function(a) {a == atoms[0] ? a : a + "+" }, atoms.reverse())
    var contradiction = reduce(function(a, acc) { acc + a }, "no.", x)

    var R1PolarResponses = filter(
      function(r) {
        // some responses can never be true
        r != "yes.nothing" && r != contradiction
      },
      sampleR1PolarResponses.support());
    return(R1PolarResponses)
  }

  var powerSet = makePowerSet(atoms);

  // return powerset, R1 licencesed responses
  var out = {
    'atoms'            : atoms,
    'powerSet'         : powerSet,
    'R1WHResponses'    : powerSet,
    'R1PolarResponses' : makeR1polarResponses(atoms, powerSet)
  }
  return(out)
}


var meaningFunction = function(world, question, response) {

  // meaning of literals / atoms
  var meaning_atomic = function(world, question, response) {
    // console.log(" *** now evaluating response: part ", response)
    if(response == '' || response == "---") {
      // assume silence has null meaning
      return true;
    }
    if(response == "nothing") {
      if(world == "nothing") {
        return true;
      }
    }
    if(world == "nothing" && question.type == 'wh') {
      return response == "nothing";
    }
    if(question.type == 'single-item') {
      return (
        response == 'yes' ? _.intersection(world.split('+'), question.queried).length > 0 :
          response == 'no' ?  _.intersection(world.split('+'), question.queried).length == 0 :
          all(function(item) {
            return _.includes(world.split('+'), item);
          }, response.split('+')));
      // return (
      //   response == 'yes' ? _.intersection(world.split('+'), question.queried).length > 0 :
      //     response == 'no' ?  _.intersection(world.split('+'), question.queried).length == 0 :
      //     console.error('Not a valid response to question:', question, response));
    } else
      if(question.type == 'polar-disjunct') {
        return (
          response == 'yes' ? _.intersection(world.split('+'), question.queried).length > 0 :
            response == 'no' ?  _.intersection(world.split('+'), question.queried).length == 0 :
            all(function(item) {
              return _.includes(world.split('+'), item);
            }, response.split('+')));
      } else if(question.type == 'wh') {
        // assume response is true when the shop contains every mentioned item
        return all(function(item) {
          return _.includes(world.split('+'), item);
        }, response.split('+'));
      } else {
        return console.error('question type not yet supported: ' + question.type);
      }
  }

  // meaning of conjunctions

  return all(
    function (r) {
      meaning_atomic(world, question, r)
    },
    _.split(response, '.')
  )

}


//  --------------------------
// | preparing context models |
//  --------------------------

/////////////////////////////////////////////////////////////
// minimal context model (exactly one element true/available)
/////////////////////////////////////////////////////////////

// minimal atoms
var bakedGoodsMinimal = ['RP', 'LC', 'SC', 'AS'];

var pieCakeContextMinimal = {
  name : "minimal",
  // worlds include all possible sub-sets of 0 < k < N pies and cakes
  worlds : bakedGoodsMinimal,

  // actions include ordering 1 pie/cake
  actions: _.concat(bakedGoodsMinimal, "no-order"),

  // questions include yes/no question for each baked good
  questions: [
    // single item polar questions
    {type: 'polar-disjunct', queried: ['RP'], text: 'RP?'},
    {type: 'polar-disjunct', queried: ['LC'], text: 'LC?'},
    {type: 'polar-disjunct', queried: ['SC'], text: 'SC?'},
    {type: 'polar-disjunct', queried: ['AS'], text: 'AS?'},
    // all two-place disjunctions
    // {type: 'polar-disjunct', queried: ['RP', 'LC'], text: 'RPorLC?'},
    // {type: 'polar-disjunct', queried: ['RP', 'SC'], text: 'RPorSC?'},
    // {type: 'polar-disjunct', queried: ['RP', 'AS'], text: 'RPorAS?'},
    // {type: 'polar-disjunct', queried: ['LC', 'SC'], text: 'LCorSC?'},
    // {type: 'polar-disjunct', queried: ['LC', 'AS'], text: 'LCorAS?'},
    // {type: 'polar-disjunct', queried: ['SC', 'AS'], text: 'SCorAS?'},
    // wh-question
    {type: 'wh', queried: bakedGoodsMinimal, text: 'which?'},
  ],

  // assume questioner is uncertain but answerer has Delta on true world (e.g. shopkeep)
  questionerBeliefs: Categorical({vs: bakedGoodsMinimal}),
  R0PriorOverWorlds: Delta({v: 'RP'}),

  // raspberry pie is #1 preference (U=5), lemon cake is #2 preference (U=4).
  // otherwise U=1 if whatever you order is in stock and 0 if it's not in stock
  decisionProblem: function(w, a) {
    return _.includes(w, a) ? 1 : 0;
  },
  // semantics of responses (in the context of a question)
  meaning: function(world, question, response) {
    if(response == '') {
      // assume silence has null meaning
      return true;
    }
    if(question.type == 'polar-disjunct') {
      return (response == 'yes' ? _.includes(question.queried, world) :
              response == 'no' ?  (! _.includes(question.queried, world)) : 0);
    } else if(question.type == 'wh') {
      return world == response;
    } else if(question.type == 'no-question') {
      return true;
    }
    else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },
  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'polar-disjunct') {
      return ['yes', 'no'];
    } else if(question.type == 'wh') {
      return bakedGoodsMinimal;
    } else if(question.type == 'no-question') {
      return ['yes'] // TODO: check which response best to give to no question
    }
    else {
      return console.error('question type not yet supported: ' + question.type);
    }
  }
};

// amended context with unbiased questioner beliefs and no preferences over any baked good
var pieCakeContextMinimalWithPreferences = extend(
  pieCakeContextMinimal,
  {
    name: "minimal-with-preferences",
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (a == 'RP' ? 5 : a == 'LC' ? 3 : 1) : 0.0000001;
    }
  }
)

/////////////////////////////////////////////
// richer context w/ powerset of baked goods
/////////////////////////////////////////////

// full power-set baked goods examples (from presentations)
var bakedGoods = ['raspPie', 'raspCake', 'lemonPie', 'lemonCake'];
var bakeryContextSets = prepareContextSets(bakedGoods);
var setsOfBakedGoods  = bakeryContextSets.powerSet;
var R1WHResponses     = bakeryContextSets.R1WHResponses;
var R1PolarResponses  = bakeryContextSets.R1PolarResponses;

var pieCakeContext = {
  name : "pieCake-prefs",
  // worlds include all possible sub-sets of 0 < k < N pies and cakes
  worlds : setsOfBakedGoods,

  // actions include ordering 1 pie/cake
  actions: _.concat(bakedGoods, "nothing"),

  // questions include yes/no question for each baked good
  questions: [
    {type: 'single-item', queried: ['lemonPie'], text: 'Lemon pie?'},
    {type: 'single-item', queried: ['lemonCake'], text: 'Lemon cake?'},
    {type: 'single-item', queried: ['raspPie'], text: 'Raspberry pie?'},
    {type: 'single-item', queried: ['raspCake'], text: 'Raspberry cake?'},
    {type: 'polar-disjunct', queried: ['lemonPie', 'raspPie', 'lemonPie+raspPie'], text: 'Pie?'},
    {type: 'polar-disjunct', queried: ['lemonCake', 'raspCake', 'lemonCake+raspCake'], text: 'Cake?'},
    {type: 'polar-disjunct', queried: ['lemonCake', 'raspCake', 'lemonPie', 'raspPie'], text: 'Anything?'},
    // MF additional disjunctions
    {type: 'polar-disjunct', queried: ['lemonPie', 'lemonCake', 'lemonPie+lemonCake'], text: 'Anything w/ lemon?'},
    {type: 'polar-disjunct', queried: ['raspPie', 'raspCake', 'raspPie+raspCake'], text: 'Anything w/ raspberry?'},
    {type: 'polar-disjunct', queried: ['lemonPie', 'raspCake', 'lemonPie+raspCake'], text: 'LP or RC?'},
    {type: 'polar-disjunct', queried: ['lemonCake', 'raspPie', 'lemonCake+raspPie'], text: 'RP or LC?'},
    {type: 'wh', queried: ['lemonCake', 'raspCake'], text: 'Which cakes?'},
    {type: 'wh', queried: ['lemonPie', 'raspPie'], text: 'Which pies?'},
    {type: 'wh', queried: ['lemonPie', 'lemonCake', 'raspPie', 'raspCake'], text: 'Which goods?'},
  ],

  // assume questioner is uncertain but answerer has Delta on true world (e.g. shopkeep)
  questionerBeliefs: Categorical({vs: setsOfBakedGoods}),
  R0PriorOverWorlds: Delta({v: 'lemonCake'}),
  R1PriorOverWorlds: Delta({v: 'lemonCake'}),

  // raspberry pie is #1 preference (U=5), lemon cake is #2 preference (U=4).
  // otherwise U=1 if whatever you order is in stock and 0 if it's not in stock
  decisionProblem: function(w, a) {
    return _.includes(w, a) ?
      (a == 'raspPie' ? 5 :
       a == 'lemonCake' ? 3 : 1) :
      0.0000001;
  },

  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'polar-disjunct') {
      // by definition polar questions require 'yes'/'no' answer
      var answers = ['yes', 'no'].concat(question.queried);
      return answers;
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            R1WHResponses : R1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunction
};

// amended context with feature-based (additive) preferences
var pieCakeContextAdditivePreferences = extend(
  pieCakeContext,
  {
    name : "pieCake-prefs-additive",
    // feature-additive preferences: pie = 2, cake = 1, rasp = 6, lemon = 4;
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (a == 'raspPie'   ? 8/2 :
                                 a == 'raspCake'  ? 7/2 :
                                 a == 'lemonPie'  ? 6/2 :
                                 a == 'lemonCake' ? 5/2 :
                                 a == 'nothing'   ? 1/2 :
                                 console.error('unknown action')
                                ) : 0.0000001;
    }
  }
)

// amended context with feature-based (additive) preferences (reversed)
var pieCakeContextAdditivePreferencesReversed = extend(
  pieCakeContext,
  {
    name : "pieCake-prefs-additive-reversed",
    // feature-additive preferences: pie = 4, cake = 6, rasp = 1, lemon = 2;
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (a == 'raspPie'   ? 5/2 :
                                 a == 'raspCake'  ? 7/2 :
                                 a == 'lemonPie'  ? 6/2 :
                                 a == 'lemonCake' ? 8/2 :
                                 a == 'nothing'   ? 1/2 :
                                 console.error('unknown action')
                                ) : 0.0000001;
    }
  }
)

// amended context with topic preference (dough is irrelevant)
//   agent prefers raspberry over lemon and rather has nothing than lemon
var pieCakeContext_raspberry = extend(
  pieCakeContext,
  {
    name : "raspberry-pref",
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (a == 'raspPie'   ? 7/2 :
                                 a == 'raspCake'  ? 7/2 :
                                 a == 'lemonPie'  ? 1/2 :
                                 a == 'lemonCake' ? 1/2 :
                                 a == 'nothing'   ? 2/2 :
                                 console.error('unknown action')
                                ) : 0.0000001;
    }
  }
)
//   like the previous but reversed (lemon > nothing > raspberry)
var pieCakeContext_lemon = extend(
  pieCakeContext,
  {
    name : "lemon-pref",
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (a == 'raspPie'   ? 1/2 :
                                 a == 'raspCake'  ? 1/2 :
                                 a == 'lemonPie'  ? 7/2 :
                                 a == 'lemonCake' ? 7/2 :
                                 a == 'nothing'   ? 2/2 :
                                 console.error('unknown action')
                                ) : 0.0000001;
    }
  }
)


// amended context with unbiased questioner beliefs and no preferences over any baked good
var pieCakeContextUnbiasedNoPref = extend(
  pieCakeContext,
  {
    name : "pieCake-no-prefs",
    // respondent knows true answer
    R0PriorOverWorlds: Delta({v: 'lemonCake'}),
    // no preferences over baked goods
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? 1 : 0.0000001;
    }
  }
)

// amended context with biased questioner beliefs and no preferences over any baked good
var pieCakeContextBiasedNoPref = extend(
  pieCakeContextUnbiasedNoPref,
  {
    name : "pieCake-no-prefs-over-confident",
    // assume questioner is virtually certain that they have either raspberry pie
    // or lemon cake (but not both)
    questionerBeliefs: Categorical({
      vs: setsOfBakedGoods,
      ps: map(
        function (x) {
          return x == 'raspPie' ? 1000 : x == 'lemonCake' ? 1000 : 1
        },
        setsOfBakedGoods
      )
    })
  }
)

// amended context with biased, heavily pessimistic beliefs & no preferences
var pieCakeContextBiasedPessimist = extend(
  pieCakeContextUnbiasedNoPref,
  {
    name : "pieCake-no-prefs-pessimist",
    // questioner believes that the shop has likely no or very few items
    questionerBeliefs: Categorical({
      vs: setsOfBakedGoods,
      ps: map(
        function (x) {
          return (x == 'nothing' ? 10000 :
                  x.split('+').length == '1' ? 100 :
                  x.split('+').length == '2' ? 1 :
                  x.split('+').length == '3' ? 0.01 :
                  x.split('+').length == '4' ? 0.0001 :
                  0.000000001)
        },
        setsOfBakedGoods
      )
    })
  }
)

// response cost is proportional to length in words
var cost = function(response,params) {
  return _.includes(response, "---") ? 0 : params.costWeight * (response.split('+').length);
};

////////////////////////////////////////////////////////////////////
// 'tso' : 'target-same-other' example used in the first experiments
////////////////////////////////////////////////////////////////////

var tsoAtoms = ['target', 'competitor', 'sameCat', 'otherCat']

var tsoContextSets      = prepareContextSets(tsoAtoms);
var tsoPowerSet         = tsoContextSets.powerSet;
var tsoR1WHResponses    = tsoContextSets.R1WHResponses;
var tsoR1PolarResponses = tsoContextSets.R1PolarResponses;
// var tsoR1PolarResponses = [
//   'no.---',
//   'no.competitor',
//   'no.competitor+sameCat',
//   'no.otherCat',
//   'no.competitor+sameCat+otherCat'
// ]

var tsoContext = {
  name : "tsoContext",
  worlds : tsoPowerSet,
  actions: _.concat(tsoAtoms, "nothing"),
  questions: [
    {type: 'single-item', queried: ['target'], text: 'Target?'},
    {type: 'single-item', queried: ['competitor'], text: 'Competitor?'},
    {type: 'single-item', queried: ['sameCat'], text: 'Same Cagegory?'},
    {type: 'single-item', queried: ['otherCat'], text: 'Other Category?'},
    {type: 'wh', queried: tsoAtoms, text: 'Which elements?'},
  ],
  questionerBeliefs: Categorical({vs: tsoPowerSet}),
  R0PriorOverWorlds: Delta({v: 'competitor+sameCat+otherCat'}),
  R1PriorOverWorlds: Delta({v: 'competitor+sameCat+otherCat'}),
  decisionProblem: function(w, a) {
    return _.includes(w, a) ?
                     (a == 'target'     ? 7 :
                      a == 'competitor' ? 6  :
                      a == 'sameCat'    ? 4  :
                                          1) :
               0.0001;
  },

  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'polar-disjunct') {
      // by definition polar questions require 'yes'/'no' answer
      var answers = ['yes', 'no'].concat(question.queried);
      return answers;
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            tsoR1WHResponses : tsoR1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunction
};

///////////////////////// PTs context extensions for cost-sensitive QA reasoning /////////////////
// the alternatives are constructed by <property>_<item>
// depending on context, the speaker may or may not reason about the property and respective cost
var costAtoms = ['noParking_coffee', 'parking_coffee', 'grocery']
// manually construct possible responses
var costR1PolarResponses = [
   'yes.---',
   'yes.noParking_coffee',
   'yes.parking_coffee',
   'yes.noParking_coffee+parking_coffee',
   'yes.grocery',
   'yes.noParking_coffee+parking_coffee+grocery',
]
var costR1WHResponses = ['noParking_coffee+parking_coffee+grocery']
// construct the set of possible worlds
var costPowerSet = map(
      function(v){return v.join('+');}, powerset(costAtoms)
    )
//var costPowerSet = makePowerSet(costAtoms)

var costContext = {
  name : "costContext",
  worlds : filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    costPowerSet
  ),
  type: ['parking', 'noParking'],
  actions: _.concat(costAtoms, "coffee", "nothing"), // assume the simple 'coffee' option is the one without details, 'nothing' is just remaining silent
  questions: [
    {type: 'single-item', queried: ['coffee'], text: 'Coffee?'}, // assume this is the null cost questioner
    {type: 'single-item', queried: ['noParking_coffee'], text: 'Coffee by foot?'}, // assume this is the pedestrian
    {type: 'single-item', queried: ['parking_coffee'], text: 'Coffee by car?'}, // assume this is the car driver
    {type: 'single-item', queried: ['grocery'], text: 'Grocery?'},
    {type: 'wh', queried: costAtoms, text: 'Which elements?'},
  ],
  questionerBeliefs: Categorical({vs: filter(
    function(x) {
      if (x != "") {
        return x
      }
    },
    costPowerSet
  )}),
  R0PriorOverWorlds: Delta({v: 'parking_coffee+noParking_coffee+grocery'}),
  R1PriorOverWorlds: Delta({v: 'parking_coffee+noParking_coffee+grocery'}),
  decisionProblem: function(w, a, type) { //updated DP based on considerations including both features
    return _.includes(w, a) ?
                     (_.includes(a.split('_'), type)     ? (a.split('_')[1] == 'coffee' ? 7 : 1) :
                                                           (a.split('_')[1] == 'coffee' ?  4 : 0.1)) :
               0.0001;
  },

  // R0 chooses among responses licensed by the question
  getLicensedResponsesR0: function(question) {
    if(question.type == 'single-item') {
      // by definition polar questions require 'yes'/'no' answer
      return ['yes', 'no'];
    } else if(question.type == 'wh') {
      // 'wh' questions allow you to say any set of queried items,
      // or to say "nothing" when none of the querried items exist
      return replaceEmptyListWithStringNothing(
        map(
          function(v){return v.join('+');},
          powerset(question.queried)
        ));
    } else {
      return console.error('question type not yet supported: ' + question.type);
    }
  },

  // R1 chooses among responses licensed by the question
  getLicensedResponsesR1: function(question) {
    return (question.type == 'wh' ?
            costR1WHResponses : costR1PolarResponses)
  },
  // semantic meaning function
  meaning: meaningFunction // uses the original tso one
};
//////////////////////////////////////////////////////////////////////////// 

////////////////////////////////////////////////////////////////////////////
//  -----------
// | Q&A model |
//  -----------
////////////////////////////////////////////////////////////////////////////

// soft-max choice for EU given beliefs and utils in DP
// yields the 'action policy' of the decision maker
var getActionPolicy = function(beliefs, context, params) {
  var actPol = Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    var decisionProblem = context.decisionProblem;
    var EU = expectation(beliefs, function(world) {
      decisionProblem(world,action)
    })
    factor(params.policyAlpha * EU);
    return action;
  });
  return actPol
};

///////////////////////// updated policy with by-cost-type differences /////////////////////////
var getCostActionPolicy = function(beliefs, context, type, params) {
  var actPol = Infer({method: 'enumerate'}, function() {
    var action = uniformDraw(context.actions);
    // draw type of context
    //var type = uniformDraw(context.type);
    var decisionProblem = context.decisionProblem;
    var EU = expectation(beliefs, function(world) {
      decisionProblem(world,action, type)
    })
    factor(params.policyAlpha * EU);
    return {action: action, type: type};
  });
  return actPol
};
///////////////////////////////////////////////////////////////////////////////////////////////

// returns TRUE if a response is contradictory in the light of the question
// example: "Do you have any pie?" - "No, we have lemon pie."
var isContradiction = function(context, question, response) {
  var meaning = context.meaning;
  var isContra = all(
    function(world) {
      !meaning(world,question,response)
    },
    context.worlds
  )
  // if (isContra) {
  //   console.log("Contradiction hit: ", question.text, response)
  // }
  return isContra
}

// gives updated beliefs about world state after hearing response to question
// based on a literal interpretation of the response
var updateBeliefs = function(beliefs, question, response, context) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    condition(meaning(world, question, response));
    return world;
  });
};

// utility of a question is equal to the expected _value_ of the
// DP after receiving a response minus the cost:
// U(Q)
// _value_ depends on DM's action policy
var questionUtility = function(utterance, beliefs, context, params) {
  // questioner wants to *maximize* expected payoff under decision problem
  var decisionProblem = context.decisionProblem;
  //var actionPolicy    = getActionPolicy(beliefs, context, params);
  var actionPolicy    = getCostActionPolicy(beliefs, context, utterance.split("_")[0], params);
  var actionUtility   = expectation(actionPolicy, function(action, type) { // function(action)
    // weight possible actions proportional to reward
    return expectation(beliefs, function(world) {
      return decisionProblem(world, action, type); // decisionProblem(world, action);
    });
  });
  return actionUtility - cost(utterance, params);
};

// DEPRECATED: currently only used in (deprecated) R1Sampler
// // responder wants to bring questioner's beliefs and/or action policy
// // as close to their own as possible
// var answerUtility = function(utterance, beliefs1, beliefs2, context, relevanceBeta, params) {
//   // respondent wants to *minimize* KL b/w beliefs (bringing closer to own belief)
//   var epistemicUtility = -KL(beliefs1, beliefs2);
//   var actionUtility = -KL(getActionPolicy(beliefs1, context, params),
//                           getActionPolicy(beliefs2, context, params));
//   return ((1 - relevanceBeta) * epistemicUtility
//           + relevanceBeta * actionUtility
//           - cost(utterance, params));
// };

// base-level respondent chooses any safe and true answer
// with equal probability;
// by construction, the function getLicensedResponseR0(question)
// is the set of all safe and true answers
var R0 = cache(function(question, context, params) {
console.log('--------R0-----------', context)

  var R0BeliefSupport = context.R0PriorOverWorlds.support();  // this is always a Delta-belief
  //console.log(context.R0PriorOverWorlds);
  //console.log('R0BeliefSupport ', R0BeliefSupport)
  var world = R0BeliefSupport[0];
  console.log('world ', world)
  var getLicensedResponsesR0 = context.getLicensedResponsesR0;
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(getLicensedResponsesR0(question));
    var meaning = context.meaning;
    condition(meaning(world, question, response))
    return response;
  });
});

// dummy R0 with response set of R1
var R1ContextFree = cache(function(question, context, params) {
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  // console.log("R1 dummy, licensed responses: ", responses)
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
    // console.log(response);
    var ownBeliefs = context.R0PriorOverWorlds;
    var otherBeliefs = updateBeliefs(context.questionerBeliefs, question, response, context);
    // console.log('updated beliefs: ');
    // terminalViz(otherBeliefs, 4);
    factor(10 * (-KL(ownBeliefs, otherBeliefs) - cost(response, params)));
    return response;
  });
});

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation of the response (as emitted by R0)
var updateBeliefsPragmatic = function(beliefs, question, response, context, params) {
  // var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
   // console.log('updateBeliefPragmatic ', world);
    var pragmaticResponse = R0(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// gives updated beliefs about world state after hearing response to question
// based on a pragmatic interpretation (as emitted by R0 w/ full answer set of R1)
var updateBeliefsPragmaticR1 = function(beliefs, question, response, context, params) {
  var meaning = context.meaning;
  return Infer({method: 'enumerate'}, function() {
    var world = sample(beliefs);
    var pragmaticResponse = R1ContextFree(question, extend(context, {
      R0PriorOverWorlds: Delta({v: world})
    }), params);
    observe(pragmaticResponse,response)
    return world;
  });
};

// Q1 selects a question with prob proportional to the expected
// value of the DP after hearing a response
var Q1 = function(context, params) {
console.log('----------- Q1------------');
  return Infer({method: 'enumerate'}, function(){
    var question = uniformDraw(context.questions);
    console.log('considering question', question.queried);
    var expectedUtility = expectation(context.questionerBeliefs, function(trueWorld) {
      //console.log('Q1 in possible world', trueWorld);
      var possibleResponses = R0(question, extend(context, {
        R0PriorOverWorlds: context.R0PriorOverWorlds //elta({v: trueWorld})
      }), params)
      console.log('respondent: ', possibleResponses)
      return expectation(possibleResponses, function(response) {
        console.log('considering response ', response );
        var currBeliefs = context.questionerBeliefs;
       // console.log('currBeliefs ', currBeliefs);
        var updatedBeliefs = updateBeliefsPragmatic(currBeliefs, question, response, context, params);
        return questionUtility(response, updatedBeliefs, context, params);
      });
    });
    //     console.log('expected utility: ',expectedUtility)
    var questionCost = question.type == 'no-question' ? 0 : params.questionCost;
    factor(params.questionerAlpha * (expectedUtility - questionCost));
    return question.text;
  });
};

//////////////////////////////////////////////////
// R1 : pragmatic respondent
// ---
// R1's prior beliefs are a distribution over
// different context which differ only wrt
// the questioner's beliefs and/or preferences.
// There are two kinds of R1: a sampler and an
// averager. The former is computationally faster,
// the latter is "normatively correct".
//////////////////////////////////////////////////

// R1 prior beliefs over various contexts

// R1 does not know the belief state of the questioner
var R1PriorContext_beliefsQ1 = {
  confident: pieCakeContextBiasedNoPref,
  unbiased:  pieCakeContextUnbiasedNoPref,
  pessimist: pieCakeContextBiasedPessimist,
  distribution: Categorical({vs: ["confident" , "unbiased",  "pessimist"]})
}

// R1 does not know the preferences of the questoiner
var R1PriorContext_PreferenceQ1 = {
  raspCake:  pieCakeContextAdditivePreferences,
  PieLemon:  pieCakeContextAdditivePreferencesReversed,
  distribution: Categorical({vs: ["raspCake" , "PieLemon"]})
}

// R1 does not know the preferences of the questoiner
var R1PriorContext_BinaryPrefs = {
  raspberry: pieCakeContext_raspberry,
  lemon    : pieCakeContext_lemon,
  distribution: Categorical({vs: ["raspberry" , "lemon"]})
}

var R1ContextPosterior = cache(function(context, question, R1PriorContext, params) {
  Infer(
    {method:'enumerate'},
    function() {
      var context_label  = sample(R1PriorContext.distribution);
      console.log("context_label ", context_label);
      var context_sample = extend(
        R1PriorContext[context_label],
        {R1PriorOverWorlds: context.R1PriorOverWorlds});
      //console.log('context_sample ', context_sample);
      var questioner = Q1(context_sample, params);
      // console.log("considering context: ", context_label);
      console.log("prob of question ", question.text, ": ", questioner.score(question.text));
      factor(questioner.score(question.text));
      return {label: context_label, sample: context_sample, name: context_sample.name}
    }
  )
})

// // R1Sampler is a sampling-based non-normative agent
// var R1Sampler = cache(function(context, R1PriorContext, question, params) {
//   var getLicensedResponsesR1 = context.getLicensedResponsesR1;
//   var responses = filter(function(r) {!isContradiction(context, question, r)},
//                          getLicensedResponsesR1(question));
//   return Infer({method: 'enumerate'}, function(){
//     var context_label  = sample(R1PriorContext.distribution);
//     var context_sample = extend(
//       R1PriorContext[context_label],
//       {R1PriorOverWorlds: context.R1PriorOverWorlds});
//     var questioner = Q1(context_sample, params);
//     factor(questioner.score(question.text));
//     var response = uniformDraw(responses);
//     var ownBeliefs = context.R1PriorOverWorlds;
//     var otherBeliefs = updateBeliefs(context_sample.questionerBeliefs, question, response, context_sample);
//     var utility = answerUtility(response, ownBeliefs, otherBeliefs, context_sample, params.relevanceBetaR1, params);
//     factor(params.R1Alpha * utility);
//     return {context_label, response};
//   });
// });

// R1 Averager is full rational reasoner, integrating over all relevant uncertainty
// key assumptions:
// - R1 assumes that their responses will be exhaustified (currently with fixed alpha = 10);
//   this assumption is very important, and also deals with "unawareness": not choosing anything
//   that wasn't mentioned blindly
var R1Averager = cache(function(context, R1PriorContext, question, params) {
  console.log("R1 averager ---------------\nquestion given: ", question);
  var getLicensedResponsesR1 = context.getLicensedResponsesR1;
  var responses = filter(function(r) {!isContradiction(context, question, r)},
                         getLicensedResponsesR1(question));
  console.log("available responses R1 Averager: ", responses);
  var ownBeliefs = context.R1PriorOverWorlds;
  console.log('ownBeliefs R1Averager ', ownBeliefs)
  var contextPosterior = marginalize(R1ContextPosterior(context, question,
                                                        R1PriorContext, params), 'sample');
  console.log("R1 Averager contextPosterior ", terminalViz(contextPosterior));                                                     
  return Infer({method: 'enumerate'}, function(){
    var response = uniformDraw(responses);
     // console.log("\nconsidering response: ", response);
    var expectedUtility = expectation(
      contextPosterior,
      function(context_sample) {
        console.log("Context sample: ", context_sample);
        var otherBeliefs = updateBeliefsPragmaticR1(context_sample.questionerBeliefs,
                                                    question, response, context_sample, params);
         // console.log("updated beliefs after response: ", response)
         // terminalViz(otherBeliefs, 4)
        var decisionProblem = context_sample.decisionProblem;
        // var actionPolicy = getActionPolicy(otherBeliefs, context, params);
        var actionPolicy = getCostActionPolicy(otherBeliefs, context, question.queried[0].split("_")[0], params); // TODO this defintion prob is really rigid
        console.log("action policy: ")
        terminalViz(actionPolicy, 4)
        var actionUtility = expectation(actionPolicy, function(action, type) { // function(action)
          // weight possible actions proportional to reward
          return expectation(otherBeliefs, function(world) {
            return decisionProblem(world, action, type); // decisionProblem(world, action);
          });
        });
        var EU = ((1-params.relevanceBetaR1) * -KL(ownBeliefs, otherBeliefs) +
                  params.relevanceBetaR1 * actionUtility -
                  cost(response,params))
        return (EU)
      }
    )
    factor(params.R1Alpha * expectedUtility);
    return response;
  });
});

//////////////////////////////////////////////////
// testing & debugging w/ terminal
//////////////////////////////////////////////////

var consoleOut = function(params) {

  var context = pieCakeContextAdditivePreferences;
  // var context = pieCakeContextUnbiasedNoPref;
  console.log('context: \t', context.name);

  var question = context.questions[2];
  console.log('question: \t', question.text)

  var world = setsOfBakedGoods[9];
  console.log(setsOfBakedGoods);
  var context_extended = extend(context, {
    R0PriorOverWorlds: Delta({v: world}),
    R1PriorOverWorlds: Delta({v: world}),
  });
  console.log('world: \t\t', world);

  var responsesR0 = context.getLicensedResponsesR0
  // console.log('R0 resp. set: \t', responsesR0(question))

  var responseR0 = responsesR0(question)[0]
  // console.log('response R0: \t', responseR0)

  var meaning = context.meaning
  // console.log('truth R0 r.: \t', meaning(world,question,responseR0));

  var responsesR1 = context.getLicensedResponsesR1
  // console.log('R1 resp. set \t: ', responsesR1(question))

  // map(
  //   function(world) {
  //     console.log("In world ('"+ world +"') the following of R1's responses are true: \n",
  //                 filter(function (r) {
  //                   meaning(world,question,r)
  //                 },
  //                        responsesR1(question))
  //                )
  //   },
  //   setsOfBakedGoods
  // )

  var responseR1 = responsesR1(question)[0]
  console.log('response R1: \t', responseR1)

  var meaning = context.meaning
  console.log('truth R1 r.: \t', meaning(world,question,responseR1));

  var R0_test = R0(question, context_extended, params)

  console.log("R0:")
  terminalViz(R0_test, 4)

  var R0Ext_test = R1ContextFree(question, context_extended, params)

  console.log("R0 (extended):")
  terminalViz(R0Ext_test, 4)

  // -------------------------------------
  // testing belief & preference inference
  // -------------------------------------

  // belief inference
  var R1posteriorContextBeliefs = R1ContextPosterior(pieCakeContextUnbiasedNoPref, question, R1PriorContext_beliefsQ1, params);
  console.log("Posterior inference (uncertainty about beliefs)")
  terminalViz(marginalize(R1posteriorContextBeliefs, 'label'),4)

  var R1SamplerBeliefInference  = marginalize(R1Sampler(context_extended, R1PriorContext_beliefsQ1, question, params), 'response')
  console.log("R1-Sampler (uncertainty about beliefs):")
  terminalViz(R1SamplerBeliefInference,4)

  var R1AveragerBeliefInference = R1Averager(context_extended, R1PriorContext_beliefsQ1, question, params)
  console.log("R1-Averager (uncertainty about beliefs):")
  terminalViz(R1AveragerBeliefInference,4)

  // preference inference
  var R1posteriorContextPreference = R1ContextPosterior(pieCakeContextUnbiasedNoPref, question, R1PriorContext_PreferenceQ1, params);
  console.log("Posterior inference (uncertainty about preferences)")
  terminalViz(marginalize(R1posteriorContextPreference, 'label'),4)

  var R1SamplerPreferenceInference  = marginalize(R1Sampler(context_extended, R1PriorContext_PreferenceQ1, question, params), 'response')
  console.log("R1-Sampler (uncertainty about preferences):")
  terminalViz(R1SamplerPreferenceInference,4)

  var R1AveragerPreferenceInference = R1Averager(context_extended, R1PriorContext_PreferenceQ1, question, params)
  console.log("R1-Averager (uncertainty about preferences):")
  terminalViz(R1AveragerPreferenceInference,4)

  // binary preference inference

  var R1posteriorContextPreferenceBinaryQRP = R1ContextPosterior(
    pieCakeContextUnbiasedNoPref,
    context.questions[2],
    R1PriorContext_BinaryPrefs,
    params);
  console.log("Posterior inference (binary after RP?)")
  terminalViz(marginalize(R1posteriorContextPreferenceBinaryQRP, 'label'),4)

  var R1AveragerPreferenceInferenceBinaryQRP = R1Averager(
    context_extended,
    R1PriorContext_BinaryPrefs,
    context.questions[2],
    params)
  console.log("R1-Averager (binary preferences & 'RP?'):")
  terminalViz(R1AveragerPreferenceInferenceBinaryQRP,4)

}


//////////////////////////////////////////////////
// visualization in the browser
//////////////////////////////////////////////////

// viz(R0(question, context_extended, params));
// viz(Q1(context, params));
// viz(R1Sampler);
// viz(R1Averager);

//////////////////////////////////////////////////
// output for R script
//////////////////////////////////////////////////

// function to call when using RwebPPL
var makeR = function(){

  console.log("Starting task: ", RInput[0].task, RInput[0].task == "safeAnswererPositive")

  if (RInput[0].task == "TSO") {
    // target-same-other setting (1st pilot)

    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   //
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }

    console.log('before: ', paramsGlobal)
    console.log('after: ', params)

    var contextLocal = extend(
      tsoContext,
      {decisionProblem : function(w, a) {
        return _.includes(w, a) ?
          (a == 'target' ? RInput[0].utilTarget :
           a == 'competitor' ? RInput[0].utilCompetitor :
           a == 'sameCat' ? RInput[0].utilSameCat : RInput[0].utilOtherCat) :
          0.0000001;
      }}
    )
    console.log('local context: ', contextLocal)
    var question = contextLocal.questions[0];

    // R1 knows full context model
    var R1Prior = {
      trueWorld: contextLocal,
      distribution: Categorical({vs: ["trueWorld"]})
    }
    console.log("R1Prior: ", R1Prior)
    // var R1posterior = R1ContextPosterior(contextLocal, question, R1Prior, params);
    // console.log("Posterior inference:")
    // terminalViz(marginalize(R1posterior, 'label'),4)

    var R1Prediction = R1Averager(contextLocal, R1Prior, question, params)
    console.log("R1Prediction: ", R1Prediction)
    var R1PredictionReduced = Infer({
      method: 'enumerate'},
      function() {
        var response = sample(R1Prediction);
        var match = response == 'no.---' ? true :
            response == 'no.competitor' ? true :
            response == 'no.competitor+sameCat' ? true :
            response == 'no.otherCat' ? true :
            response == 'no.competitor+sameCat+otherCat' ? true : false
        condition(match)
        return(response)
      })
    console.log("R1-Averager:")
    terminalViz(R1PredictionReduced,4)

    return(R1PredictionReduced)
  }
  if (RInput[0].task == "cost") {
    // target-same-other setting (1st pilot)

    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   //
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }

    console.log('before: ', paramsGlobal)
    console.log('after: ', params)

    var contextLocal = extend(
      costContext,
      //{decisionProblem : function(w, a) {
      //  return _.includes(w, a) ?
      //    (a == 'target' ? RInput[0].utilTarget :
      //     a == 'competitor' ? RInput[0].utilCompetitor :
      //     a == 'sameCat' ? RInput[0].utilSameCat : RInput[0].utilOtherCat) :
      //    0.0000001;
      //}}
      {decisionProblem: costContext.decisionProblem}
    )
    console.log('local context: ', contextLocal)
    var question = contextLocal.questions[2]; // TODO: add available responses for "coffee?" question
    console.log('question ', question)
    
    // R1 knows full context model
    var R1Prior = {
      trueWorld: contextLocal,
      distribution: Categorical({vs: ["trueWorld"]})
    }
    console.log("R1Prior: ", R1Prior)

    var R1Prediction = R1Averager(contextLocal, R1Prior, question, params)
    console.log("R1Prediction: ", R1Prediction)
    var R1PredictionReduced = Infer({
      method: 'enumerate'},
      function() {
        var response = sample(R1Prediction);
       // var match = response == 'no.---' ? true :
       //      response == 'no.competitor' ? true :
       //     response == 'no.competitor+sameCat' ? true :
       //      response == 'no.otherCat' ? true :
       //     response == 'no.competitor+sameCat+otherCat' ? true : false
        var match = response == 'yes.---' ? true :
            response == 'yes.noParking_coffee' ? true :
            response == 'yes.parking_coffee' ? true :
            response == 'yes.noParking_coffee+parking_coffee' ? true :
            response == 'yes.grocery' ? true :
            response == 'yes.noParking_coffee+parking_coffee+grocery' ? true : false    
        condition(match)
        return(response)
      })
    console.log("R1-Averager:")
    terminalViz(R1PredictionReduced,4)

    return(R1PredictionReduced)
  }

  if (RInput[0].task == "continuousInference") {
    // two-parameter continuous inference of preferences
    var cI = getContInf()
    console.log("CI")
    return cI
  }

  if (RInput[0].task == "safeAnswererNegative") {
    // R0 (safe answerer) giving a negative response to disjunctive question
    var context = pieCakeContextUnbiasedNoPref // neutral context
    var question = context.questions[7]; // anything w/ raspberry?
    var world = setsOfBakedGoods[11];     // RP+LC
    console.log("Context: " , context.name)
    console.log("Question: " , question.text)
    console.log("World: " , world)
    var context_extended = extend(context, {
      R0PriorOverWorlds: Delta({v: world}),
      R1PriorOverWorlds: Delta({v: world}),
    });
    var R0test = R0( question,  context_extended, params )
    terminalViz(R0test)
    return R0test
  }

  if (RInput[0].task == "safeAnswererPositive") {
    // R0 (safe answerer) giving a positive response to disjunctive question
    var context = pieCakeContextUnbiasedNoPref // neutral context
    var question = context.questions[8]; // anything w/ raspberry?
    var world = setsOfBakedGoods[9];     // RP+LC
    console.log("Context: " , context.name)
    console.log("Question: " , question)
    console.log("World: " , world)
    var context_extended = extend(context, {
      R0PriorOverWorlds: Delta({v: world}),
      R1PriorOverWorlds: Delta({v: world}),
    });
    var params =
      {
        policyAlpha     : RInput[0].policyAlpha,
        questionerAlpha : RInput[0].questionerAlpha,
        R1Alpha         : RInput[0].R1Alpha,
        relevanceBetaR0 : RInput[0].relevanceBetaR0,  // beta=1 for only action-utility
        relevanceBetaR1 : RInput[0].relevanceBetaR1,   //
        costWeight      : RInput[0].costWeight,
        questionCost    : RInput[0].questionCost      // cost for a question (relative to no question)
      }
    var R0test = R0( question,  context_extended, params )
    terminalViz(R0test)
    return R0test
  }

  if (RInput[0].task == "R1Responses_BinaryPrefs") {
    // R1 (averager) response selection
    var context = pieCakeContextUnbiasedNoPref // neutral context
    var question = context.questions[2]; // RP?
    var world = setsOfBakedGoods[9];     // RP+LC
    var context_extended = extend(context, {
      R0PriorOverWorlds: Delta({v: world}),
      R1PriorOverWorlds: Delta({v: world}),
    });
    return R1Averager(
      context_extended,
      R1PriorContext_BinaryPrefs,
      question,
      params)
  }
  if (RInput[0].task == "R1Posterior_BinaryPrefs") {
    // R1 posterior over contexts for binary preference inference
    var context = pieCakeContextUnbiasedNoPref // neutral context
    var question = context.questions[2]; // RP?
    var world = setsOfBakedGoods[9];     // RP+LC
    var context_extended = extend(context, {
      R0PriorOverWorlds: Delta({v: world}),
      R1PriorOverWorlds: Delta({v: world}),
    });
    return marginalize(R1ContextPosterior(
      context,
      question,
      R1PriorContext_BinaryPrefs,
      params
    ), 'label');
  }
  else {
    var context =
    RInput[0].task == 'pieCakeContextMinimal' ? pieCakeContextMinimal :
    RInput[0].task == 'pieCakeContextMinimalWithPreferences' ? pieCakeContextMinimalWithPreferences :
    RInput[0].task == 'pieCakeContext' ? pieCakeContext :
    RInput[0].task == 'pieCakeContextAdditivePreferences' ? pieCakeContextAdditivePreferences :
    RInput[0].task == 'pieCakeContextBiasedNoPref' ? pieCakeContextBiasedNoPref :
    RInput[0].task == 'pieCakeContextUnbiasedNoPref' ? pieCakeContextUnbiasedNoPref :
    RInput[0].task == 'pieCakeContextBiasedPessimist' ? pieCakeContextBiasedPessimist :
        false
    Q1(context, params)
  }
}

//////////////////////////////////////////////////
// inferring continuous preferences
// ///////////////////////////////////////////////

// build a context representation for a given pair of
// payoff parameters
var makeContext = function(RP, LC) {
  extend(
    pieCakeContextMinimal,
    {
      questions: [
        {type: 'polar-disjunct', queried: ['RP'], text: 'RP?'},
        {type: 'polar-disjunct', queried: ['LC'], text: 'LC?'},
        // {type: 'wh', queried: bakedGoodsMinimal, text: 'which?'},
        {type: 'no-question', queried: [], text: 'no-Q'}
      ],
      decisionProblem: function(w, a) {
        return a == 'no-order' ? 0 :
        _.includes(w, a) ? (
          a == 'RP' ? RP  : RP+LC) : -1;
      }
    });
}

var getContInf = function(params){

  var getScore = function(RP, LC) {
    // build context with parameters
    var c =  makeContext(RP, LC);
    // questioner choice & score
    var questioner = Q1(c, params)
    var score = questioner.score('RP?')
    return(score)
  }

  var contInf = Infer(
    {method: "MCMC", samples: 4000},
    function() {
      // RP is the reference level
      var RP = gaussian({mu: 2, sigma: 2});;
      // 'coefficients': deviation from reference level
      var LC = gaussian({mu: 0, sigma: 1});
      factor(getScore(RP,LC))
      var out = {RP: RP, LC: LC};
      return (out)
    }
  )

  return(contInf)

}

var continuousInference = function(params){
  var cI = getContInf(params)
  viz(cI)
  viz(marginalize(cI, "RP"))
  viz(marginalize(cI, "LC"))
}

var consoleOutQ1ContInf = function(RP, LC, params) {
  console.log('question probs with: RP =', RP, 'LC =', LC)
  terminalViz(Q1(makeContext(RP,LC), params))
}


////////////////////////////////////////////////////////////////////////////
// dev corner
////////////////////////////////////////////////////////////////////////////

var consoleOutTSO = function(params) {

  var context = tsoContext;
  console.log('context: \t', context.name);

  var question = context.questions[0];
  console.log('question: \t', question.text)

  var world = 'competitor+sameCat+otherCat1+otherCat2';
  console.log('world: \t\t', world);

  var responsesR0 = context.getLicensedResponsesR0
  console.log('R0 resp. set: \t', responsesR0(question))

  var responseR0 = responsesR0(question)[0]
  console.log('response R0: \t', responseR0)

  var meaning = context.meaning
  console.log('truth R0 r.: \t', meaning(world,question,responseR0));

  var responsesR1 = context.getLicensedResponsesR1
  // console.log('R1 resp. set \t: ', responsesR1(question))

  // map(
  //   function(world) {
  //     console.log("In world ('"+ world +"') the following of R1's responses are true: \n",
  //                 filter(function (r) {
  //                   meaning(world,question,r)
  //                 },
  //                        responsesR1(question))
  //                )
  //   },
  //   setsOfBakedGoods
  // )

  var responseR1 = responsesR1(question)[0]
  console.log('response R1: \t', responseR1)

  var meaning = context.meaning
  console.log('truth R1 r.: \t', meaning(world,question,responseR1));

  var R0_test = R0(question, context, params)

  console.log("R0:")
  terminalViz(R0_test, 4)

  var Q1_test = Q1(context, params)
  console.log("Q1:")
  terminalViz(Q1_test, 4)

  // var R0Ext_test = R1ContextFree(question, context, params)
  // console.log("R0 (extended):")
  // terminalViz(R0Ext_test, 4)

  // ---------------------------------------------
  // testing R1 with full knowledge of preferences
  // ---------------------------------------------

  // R1 knows full context model
  var R1Prior_FullKnowledge = {
    trueWorld: tsoContext,
    distribution: Categorical({vs: ["trueWorld"]})
  }

  var R1posterior = R1ContextPosterior(tsoContext, question, R1Prior_FullKnowledge, params);
  console.log("Posterior inference (trivial)")
  terminalViz(marginalize(R1posterior, 'label'),4)

  var R1Prediction = R1Averager(tsoContext, R1Prior_FullKnowledge, question, params)
  var R1PredictionReduced = Infer({
    method: 'enumerate'},
    function() {
      var response = sample(R1Prediction);
      var match = response == 'no.---' ? true :
          response == 'no.competitor' ? true :
          response == 'no.competitor+sameCat' ? true :
          response == 'no.otherCat' ? true :
          response == 'no.competitor+sameCat+otherCat' ? true : false
      condition(match)
      return(response)
    })
  console.log("R1-Averager (full knowledge):")
  terminalViz(R1PredictionReduced,4)

}

////////////////////////////////////////////////////////////////////////////
// control center      : main functions to call
// ---
// makeR               : use in connection with 'collect-webppl-results.r'
// consoleOut          : use w/ terminal for dev
// continuousInference : use in webppl.org (plotting) of contin. Inference
// consoleOutQ1Cont    : use w/ terminal for dev Q1 beh. in cont. Inference
// consoleOutTSO       : use w/ terminal for dev of TSO case (1st pilot)
// /////////////////////////////////////////////////////////////////////////

//  -------------------
// | global parameters |
//  -------------------

var paramsGlobal = {
  policyAlpha     : 2.5,    // SM-alpha for action policy
  questionerAlpha : 4,      // SM-alpha for question choice
  R1Alpha         : 3,      // SM-alpha for R1
  relevanceBetaR0 : 0,      // beta=1 for only action-utility
  relevanceBetaR1 : 0.95,   //
  costWeight      : 2.5,
  questionCost    : 0.25    // cost for a question (relative to no question)
};


makeR()
// consoleOut(paramsGlobal)
// continuousInference(paramsGlobal)
// consoleOutQ1ContInf(5,-1, paramsGlobal)

// consoleOutTSO(paramsGlobal); // compare against pilot data from N=50
          // frequency of answer types:
          // 0.1267 0.5698 0.2068 0.0000 0.0967
